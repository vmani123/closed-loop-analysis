{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tkinter import Tk, filedialog\n",
    "import ast\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "import pprint \n",
    "import json\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tk root\n",
    "root = Tk()\n",
    "\n",
    "# Hide the main window\n",
    "root.withdraw()\n",
    "\n",
    "# Set the main window to be always on top\n",
    "root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "\n",
    "# Ask user to select CSV files\n",
    "csv_paths = filedialog.askopenfilename(multiple=True, title='Select CSV files', filetypes=[('CSV files', '*.csv')])\n",
    "\n",
    "# Initialize a list to store DataFrames for each selected CSV file\n",
    "data_frames = []\n",
    "\n",
    "# Iterate over each selected CSV file path\n",
    "for csv_path in csv_paths:\n",
    "    df = pd.read_csv(csv_path, sep=';')\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "combined_df.update(combined_df.applymap(lambda x: x.strip() if isinstance(x, str) else x))\n",
    "\n",
    "# Now combined_df contains data from all selected CSV files\n",
    "#print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available headers:\n",
      "0: Mouse\n",
      "1: Genotype\n",
      "2: Date\n",
      "3: Selected Behavior\n",
      "4: Sham\n",
      "5: Session Number\n",
      "6: Fiber Connection\n",
      "7: Right Turn Array\n",
      "8: Right Turn Array Continue\n",
      "9: Total Right Turn Bout Count\n",
      "10: Total Right Turn Stim Count\n",
      "11: Average Duration of Right Turn(seconds)\n",
      "12: Total Right Turn Stim in Stim Block\n",
      "13: Total Right Turn Bouts in Stim Block\n",
      "14: Right Turn Average Duration in Stim Block(seconds)\n",
      "15: Locomotion Array\n",
      "16: Locomotion Array Continue\n",
      "17: Total Locomotion Bout Count\n",
      "18: Total Locomotion Stim Count\n",
      "19: Average Duration of Locomotion(seconds)\n",
      "20: Total Locomotion Stim in Stim Block\n",
      "21: Total Locomotion Bouts in Stim Block\n",
      "22: Locomotion Average Duration in Stim Block(seconds)\n",
      "23: Face Groom Array\n",
      "24: Face Groom Array Continue\n",
      "25: Total Face Groom Bout Count\n",
      "26: Total Face Groom Stim Count\n",
      "27: Average Duration of Face Groom(seconds)\n",
      "28: Total Face Groom Stim in Stim Block\n",
      "29: Total Face Groom Bouts in Stim Block\n",
      "30: Face Groom Average Duration in Stim Block(seconds)\n",
      "Enter the number of the header you want to filter by: 1\n",
      "Enter the value to filter by Genotype: D1\n",
      "Matching Rows:\n",
      "42\n",
      "\n",
      "All Selections and Filtering Criteria:\n",
      "Selected Header: Genotype\n",
      "Filter Value: D1\n"
     ]
    }
   ],
   "source": [
    "# Define a list to store all selections and filtering criteria\n",
    "filtering_criteria = []\n",
    "\n",
    "print(\"Available headers:\")\n",
    "for idx, header in enumerate(combined_df.columns):\n",
    "    print(f\"{idx}: {header}\")\n",
    "\n",
    "# Prompt the user to choose a header by its number\n",
    "header_number = int(input(\"Enter the number of the header you want to filter by: \"))\n",
    "\n",
    "# Check if the selected header is \"array\" and ask for input again if it is\n",
    "while combined_df.columns[header_number] == \"array\":\n",
    "    print(\"Sorry, you cannot select the 'array' column.\")\n",
    "    header_number = int(input(\"Enter the number of the header you want to filter by: \"))\n",
    "\n",
    "selected_header = combined_df.columns[header_number]\n",
    "\n",
    "# Append the selected header to filtering_criteria\n",
    "filtering_criteria.append(f\"Selected Header: {selected_header}\")\n",
    "\n",
    "filter_value = input(f\"Enter the value to filter by {selected_header}: \")\n",
    "\n",
    "# Append the filter value to filtering_criteria\n",
    "filtering_criteria.append(f\"Filter Value: {filter_value}\")\n",
    "\n",
    "# Filtering rows based on the selected header and filter value\n",
    "matching_rows = combined_df[combined_df[selected_header] == filter_value]\n",
    "\n",
    "# Extracting the headers in the original order from the DataFrame\n",
    "original_headers = matching_rows.columns.tolist()\n",
    "\n",
    "print(\"Matching Rows:\")\n",
    "print(len(matching_rows))\n",
    "\n",
    "# Display all selections and filtering criteria\n",
    "print(\"\\nAll Selections and Filtering Criteria:\")\n",
    "for criterion in filtering_criteria:\n",
    "    print(criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to start filtering? (y/n): y\n",
      "\n",
      "Available headers:\n",
      "0: Mouse\n",
      "1: Genotype\n",
      "2: Date\n",
      "3: Selected Behavior\n",
      "4: Sham\n",
      "5: Session Number\n",
      "6: Fiber Connection\n",
      "7: Right Turn Array\n",
      "8: Right Turn Array Continue\n",
      "9: Total Right Turn Bout Count\n",
      "10: Total Right Turn Stim Count\n",
      "11: Average Duration of Right Turn(seconds)\n",
      "12: Total Right Turn Stim in Stim Block\n",
      "13: Total Right Turn Bouts in Stim Block\n",
      "14: Right Turn Average Duration in Stim Block(seconds)\n",
      "15: Locomotion Array\n",
      "16: Locomotion Array Continue\n",
      "17: Total Locomotion Bout Count\n",
      "18: Total Locomotion Stim Count\n",
      "19: Average Duration of Locomotion(seconds)\n",
      "20: Total Locomotion Stim in Stim Block\n",
      "21: Total Locomotion Bouts in Stim Block\n",
      "22: Locomotion Average Duration in Stim Block(seconds)\n",
      "23: Face Groom Array\n",
      "24: Face Groom Array Continue\n",
      "25: Total Face Groom Bout Count\n",
      "26: Total Face Groom Stim Count\n",
      "27: Average Duration of Face Groom(seconds)\n",
      "28: Total Face Groom Stim in Stim Block\n",
      "29: Total Face Groom Bouts in Stim Block\n",
      "30: Face Groom Average Duration in Stim Block(seconds)\n",
      "Enter the number of the header you want to filter by: 3\n",
      "Selected Behavior\n",
      "The substring 'Array' was not found in the header.\n",
      "Unique possible values for Selected Behavior:\n",
      "Face Groom\n",
      "Locomotion\n",
      "Right Turn\n",
      "Enter the value to filter by Selected Behavior: Locomotion\n",
      "Do you want to continue? (y/n): y\n",
      "Do you want to start filtering? (y/n): y\n",
      "\n",
      "Available headers:\n",
      "0: Mouse\n",
      "1: Genotype\n",
      "2: Date\n",
      "3: Selected Behavior\n",
      "4: Sham\n",
      "5: Session Number\n",
      "6: Fiber Connection\n",
      "7: Right Turn Array\n",
      "8: Right Turn Array Continue\n",
      "9: Total Right Turn Bout Count\n",
      "10: Total Right Turn Stim Count\n",
      "11: Average Duration of Right Turn(seconds)\n",
      "12: Total Right Turn Stim in Stim Block\n",
      "13: Total Right Turn Bouts in Stim Block\n",
      "14: Right Turn Average Duration in Stim Block(seconds)\n",
      "15: Locomotion Array\n",
      "16: Locomotion Array Continue\n",
      "17: Total Locomotion Bout Count\n",
      "18: Total Locomotion Stim Count\n",
      "19: Average Duration of Locomotion(seconds)\n",
      "20: Total Locomotion Stim in Stim Block\n",
      "21: Total Locomotion Bouts in Stim Block\n",
      "22: Locomotion Average Duration in Stim Block(seconds)\n",
      "23: Face Groom Array\n",
      "24: Face Groom Array Continue\n",
      "25: Total Face Groom Bout Count\n",
      "26: Total Face Groom Stim Count\n",
      "27: Average Duration of Face Groom(seconds)\n",
      "28: Total Face Groom Stim in Stim Block\n",
      "29: Total Face Groom Bouts in Stim Block\n",
      "30: Face Groom Average Duration in Stim Block(seconds)\n",
      "Enter the number of the header you want to filter by: 4\n",
      "Sham\n",
      "The substring 'Array' was not found in the header.\n",
      "Unique possible values for Sham:\n",
      "n\n",
      "y\n",
      "Enter the value to filter by Sham: n\n",
      "Do you want to continue? (y/n): n\n",
      "\n",
      "All Selections and Filtering Criteria:\n",
      "Selected Header: Genotype\n",
      "Filter Value: D1\n",
      "Matching Rows Count: 42\n",
      "Selected Header: Selected Behavior\n",
      "Filter Value: Locomotion\n",
      "Selected Header: Sham\n",
      "Filter Value: n\n",
      "Matching Rows Count: 6\n"
     ]
    }
   ],
   "source": [
    " def get_possible_indices(tuples):\n",
    "    index_to_label = {\n",
    "        0: \"Behavior,0\",\n",
    "        1: \"Stim Count,1\",\n",
    "        2: \"Frame Rate,2\",\n",
    "        3: \"Bouts,3\",\n",
    "        4: \"Duration,4\", \n",
    "        5: \"Start Time,5\", \n",
    "        6: \"End Time,6\"\n",
    "\n",
    "    }\n",
    "    if tuples and isinstance(tuples[0], tuple):\n",
    "        return [index_to_label[i] if i in index_to_label else f\"Unknown Label {i}\" for i in range(len(tuples[0]))]\n",
    "    return []\n",
    "\n",
    "def get_possible_indices_integers(tuples):\n",
    "    if tuples and isinstance(tuples[0], tuple):\n",
    "        return list(range(len(tuples[0])))\n",
    "    return []\n",
    "\n",
    "original_matching_rows = matching_rows.copy()\n",
    "\n",
    "while True:\n",
    "    start_filtering = input(\"Do you want to start filtering? (y/n): \")\n",
    "    if start_filtering.lower() != 'y':\n",
    "        print(\"Filtering process cancelled.\")\n",
    "        break\n",
    "    \n",
    "    print(\"\\nAvailable headers:\")\n",
    "    for idx, header in enumerate(matching_rows.columns):\n",
    "        print(f\"{idx}: {header}\")\n",
    "\n",
    "    try:\n",
    "        header_number = int(input(\"Enter the number of the header you want to filter by: \"))\n",
    "        \n",
    "        if 0 <= header_number < len(matching_rows.columns):\n",
    "            selected_header = matching_rows.columns[header_number]\n",
    "            header_words = selected_header.split()\n",
    "            filtering_criteria.append(f\"Selected Header: {selected_header}\")\n",
    "            print(selected_header)\n",
    "            \n",
    "            substring = 'Array'\n",
    "            if substring in selected_header:\n",
    "                print(f\"The substring '{substring}' was found in the header.\")\n",
    "            else:\n",
    "                print(f\"The substring '{substring}' was not found in the header.\")\n",
    "\n",
    "            if 'Array' in selected_header:\n",
    "                tuple_cells = matching_rows[selected_header].tolist()\n",
    "                #print(tuple_cells)\n",
    "                int_1 = []\n",
    "\n",
    "                for subli in tuple_cells:\n",
    "                    intermediate = subli.strip('][').split('][')\n",
    "                    int_1.append(intermediate)\n",
    "\n",
    "                tuples_list = [[tuple(float(value) if '.' in value else int(value) for value in s.split(',')) for s in inner_list] for inner_list in int_1]\n",
    "                #for x in tuples_list:\n",
    "                    #print('Tuples List Before',len(x))\n",
    "                # Find the index of selected_header in the column headers\n",
    "                if selected_header in matching_rows.columns:\n",
    "                    header_index = matching_rows.columns.get_loc(selected_header)\n",
    "\n",
    "                    # Check if the next column index is valid\n",
    "                    if header_index + 1 < len(matching_rows.columns):\n",
    "                        next_column_name = matching_rows.columns[header_index + 1]\n",
    "                        matching_columns_next = matching_rows[next_column_name]\n",
    "                        tuple_cells_2 = matching_columns_next.tolist()\n",
    "                        int_2 = []\n",
    "\n",
    "                        for subli in tuple_cells_2:\n",
    "                            if isinstance(subli, str):\n",
    "                                intermediate = subli.strip('][').split('][')\n",
    "                            elif isinstance(subli, int):\n",
    "                                intermediate = [str(subli)]\n",
    "                                \n",
    "                            int_2.append(intermediate)\n",
    "\n",
    "                    tuples_list_2 = [[tuple(float(value) if '.' in value else int(value) for value in s.split(',')) for s in inner_list] for inner_list in int_2]\n",
    "            \n",
    "                    values_added = False  # Initialize a flag to track if values were added\n",
    "\n",
    "                    # Check the length of each tuple in tuplesst_2\n",
    "                    for index, value in enumerate(tuples_list_2):\n",
    "                        if len(value)== 1:\n",
    "                            continue  # Skip single-value tuples\n",
    "\n",
    "                        # Check if the index is within the bounds of tuples_list\n",
    "                        if index < len(tuples_list):\n",
    "                            tuples_list[index].extend(value)\n",
    "                            values_added = True  # Set the flag to True if values were added\n",
    "                                    # Check if values were added and inform the user\n",
    "                    if values_added:\n",
    "                        print(\"Values were added to tuples_list from tuples_list_2.\")\n",
    "                    else:\n",
    "                        print(\"No values were added to tuples_list from tuples_list_2.\")\n",
    "                        \n",
    "                    #for x in tuples_list:\n",
    "                        #print('Tuples List After:', len(x))\n",
    "\n",
    "                row_index_to_values = {}\n",
    "                # Create a dictionary that associates row indices with values from tuples_list\n",
    "                for row_index, row_data in enumerate(matching_rows.itertuples(), start=0):\n",
    "                    row_identifier = row_data.Index  # Get the row identifier (usually the index)\n",
    "                    row_index_to_values[row_identifier] = tuples_list[row_index]\n",
    "                #for row_index, values in row_index_to_values.items():\n",
    "                    #print(f\"Row Index: {row_index}, Values: {values}\")\n",
    "\n",
    "                possible_indices = get_possible_indices(tuples_list[0])\n",
    "                print(\"\\nAvailable indices for the elements within tuples:\")\n",
    "                print(possible_indices)\n",
    "                possible_indices_integers = get_possible_indices_integers(tuples_list[0])\n",
    "                \n",
    "                selected_indices_str = input(\"Enter the indices of the elements within each tuple to filter by (comma-separated): \")\n",
    "                selected_indices = [int(idx.strip()) for idx in selected_indices_str.split(',')]\n",
    "                selected_indices_with_range = []\n",
    "                \n",
    "                for selected_index in selected_indices:\n",
    "                    if selected_index == 5 or selected_index == 6:\n",
    "                        selected_indices_with_range.append(selected_index)\n",
    "                    elif selected_index in possible_indices_integers:\n",
    "                        selected_indices_with_range.append(selected_index)\n",
    "                    else:\n",
    "                        print(f\"Invalid tuple element index {selected_index}. Skipping.\")\n",
    "                # Append selected indices to filtering_criteria\n",
    "                filtering_criteria.append(f\"Selected Indices: {selected_indices_with_range}\") \n",
    "                \n",
    "                if 5 in selected_indices_with_range or 6 in selected_indices_with_range:\n",
    "                    range_filtering = input(\"Do you want to filter by an exact value or a range for index 5 or 6? (exact/range): \").lower()\n",
    "                    if range_filtering == 'range':\n",
    "                        range_min = int(input(\"Enter the minimum value of the range: \"))\n",
    "                        range_max = int(input(\"Enter the maximum value of the range: \"))\n",
    "                        # Append selected indices to filtering_criteria\n",
    "                        filtering_criteria.append(f\"Selected Indices: {selected_indices_with_range}\")\n",
    "                        matching_tuples = {}\n",
    "                        for row_index, values in row_index_to_values.items():\n",
    "                            for current_tuple in values:\n",
    "                                value_at_selected_index = current_tuple[selected_index]\n",
    "                                if range_min <= value_at_selected_index <= range_max:\n",
    "                                    if row_index not in matching_tuples:\n",
    "                                        matching_tuples[row_index] = []\n",
    "                                        matching_tuples[row_index].append(current_tuple)\n",
    "                                    else:\n",
    "                                        matching_tuples[row_index].append(current_tuple)\n",
    "                                        \n",
    "                        if len(matching_tuples) > 0:\n",
    "                            mask = []\n",
    "                            for x in tuples_list:\n",
    "                                found_true_for_tuple = False\n",
    "\n",
    "                                for current_tuple in x:\n",
    "                                    value_at_selected_index = current_tuple[selected_index]\n",
    "                                    if range_min <= value_at_selected_index <= range_max:\n",
    "                                        mask.append(True)\n",
    "                                        found_true_for_tuple = True\n",
    "                                        break\n",
    "\n",
    "                                if not found_true_for_tuple:\n",
    "                                    mask.append(False)\n",
    "\n",
    "                            indices_to_keep = [i for i, value in enumerate(mask) if value]\n",
    "                            filtered_matching_rows = matching_rows[matching_rows.index.isin(indices_to_keep)]\n",
    "                            matching_rows = filtered_matching_rows\n",
    "                            original_matching_rows = matching_rows.copy()\n",
    "                        else:\n",
    "                            print(\"There are no matching tuples and no matching rows\")\n",
    "                            if len(matching_rows) == 0:\n",
    "                                matching_rows = original_matching_rows.copy()\n",
    "                                print('Matching Rows Data Frame Updated to Last Successful Filter')\n",
    "\n",
    "                    else:\n",
    "                        user_input = input(\"Enter the value to filter by in the selected element at index 5 or 6 (or 'all' to select all): \")\n",
    "                        if user_input.lower() == 'all':\n",
    "                            matching_tuples = {}\n",
    "                            for row_index, values in row_index_to_values.items():\n",
    "                                for current_tuple in values:\n",
    "                                    if row_index not in matching_tuples:\n",
    "                                        matching_tuples[row_index] = []\n",
    "                                        matching_tuples[row_index].append(current_tuple)\n",
    "                                    else:\n",
    "                                        matching_tuples[row_index].append(current_tuple)\n",
    "                                                \n",
    "                        else:\n",
    "                            try:\n",
    "                                user_input = int(user_input)\n",
    "                                matching_tuples = {}\n",
    "                                for row_index, values in row_index_to_values.items():\n",
    "                                    for current_tuple in values:\n",
    "                                        value_at_selected_index = current_tuple[selected_index]\n",
    "                                        if value_at_selected_index == user_input:\n",
    "                                            if row_index not in matching_tuples:\n",
    "                                                matching_tuples[row_index] = []\n",
    "                                                matching_tuples[row_index].append(current_tuple)\n",
    "                                            else:\n",
    "                                                matching_tuples[row_index].append(current_tuple)\n",
    "                                                \n",
    "                                if len(matching_tuples) > 0:\n",
    "                                    mask = []\n",
    "                                    for x in tuples_list:\n",
    "                                        found_true_for_tuple = False\n",
    "\n",
    "                                        for current_tuple in x:\n",
    "                                            value_at_selected_index = current_tuple[selected_index]\n",
    "                                            if value_at_selected_index == user_input:\n",
    "                                                mask.append(True)\n",
    "                                                found_true_for_tuple = True\n",
    "                                                break\n",
    "\n",
    "                                        if not found_true_for_tuple:\n",
    "                                            mask.append(False)\n",
    "\n",
    "                                    indices_to_keep = [i for i, value in enumerate(mask) if value]\n",
    "                                    filtered_matching_rows = matching_rows[matching_rows.index.isin(indices_to_keep)]\n",
    "                                    matching_rows = filtered_matching_rows\n",
    "                                    original_matching_rows = matching_rows.copy()\n",
    "                                    print('Matching Rows Data Frame and Original Copy Data Frame Are Updated')\n",
    "                                else:\n",
    "                                    print(\"There are no matching tuples and no matching rows\")\n",
    "                                    if len(matching_rows) == 0:\n",
    "                                        matching_rows = original_matching_rows.copy()\n",
    "                                        print('Matching Rows Data Frame Replaced with Last Successful Filter')\n",
    "\n",
    "                            except ValueError:\n",
    "                                print(f\"Invalid input for index 5 or 6. Please enter a valid value of the expected data type.\")\n",
    "\n",
    "                else:\n",
    "                    for selected_index in selected_indices_with_range:\n",
    "                        if selected_index != 5 or selected_index != 6:\n",
    "                            example_tuple = tuples_list[0]\n",
    "                            for x in example_tuple[:7]:\n",
    "                                print(\"Examples Tuples:\", x)\n",
    "\n",
    "                            values = set()\n",
    "                            for x in tuples_list:\n",
    "                                for y in x:\n",
    "                                    if len(y) > selected_index:\n",
    "                                        values.add(y[selected_index])\n",
    "                            print(f\"\\nPreview of possible values for the selected element at index {selected_index}:\")\n",
    "                            print(values)\n",
    "                            \n",
    "                            user_input = input(f\"Enter the value to filter by in the selected element at index {selected_index} (or 'all' to select all): \")\n",
    "\n",
    "                            if user_input.lower() == 'all':\n",
    "                                matching_tuples = {}\n",
    "                                for row_index, values in row_index_to_values.items():\n",
    "                                    for current_tuple in values:\n",
    "                                        if row_index not in matching_tuples:\n",
    "                                            matching_tuples[row_index] = []\n",
    "                                            matching_tuples[row_index].append(current_tuple)\n",
    "                                        else:\n",
    "                                            matching_tuples[row_index].append(current_tuple)\n",
    "                            else:\n",
    "                                try:\n",
    "                                    user_input = int(user_input)\n",
    "                                    matching_tuples = {}\n",
    "                                    for row_index, values in row_index_to_values.items():\n",
    "                                        for current_tuple in values:\n",
    "                                            value_at_selected_index = current_tuple[selected_index]\n",
    "                                            if value_at_selected_index == user_input:\n",
    "                                                if row_index not in matching_tuples:\n",
    "                                                    matching_tuples[row_index] = []\n",
    "                                                    matching_tuples[row_index].append(current_tuple)\n",
    "                                                else:\n",
    "                                                    matching_tuples[row_index].append(current_tuple)\n",
    "                                                \n",
    "                                    if len(matching_tuples) > 0:\n",
    "                                        mask = []\n",
    "                                        for x in tuples_list:\n",
    "                                            found_true_for_tuple = False\n",
    "\n",
    "                                            for current_tuple in x:\n",
    "                                                value_at_selected_index = current_tuple[selected_index]\n",
    "                                                if value_at_selected_index == user_input:\n",
    "                                                    mask.append(True)\n",
    "                                                    found_true_for_tuple = True\n",
    "                                                    break\n",
    "\n",
    "                                            if not found_true_for_tuple:\n",
    "                                                mask.append(False)\n",
    "\n",
    "                                        indices_to_keep = [i for i, value in enumerate(mask) if value]\n",
    "                                        filtered_matching_rows = matching_rows[matching_rows.index.isin(indices_to_keep)]\n",
    "                                        matching_rows = filtered_matching_rows\n",
    "                                        original_matching_rows = matching_rows.copy()\n",
    "                                    else:\n",
    "                                        print(\"There are no matching tuples and no matching rows\")\n",
    "                                        if len(matching_rows) == 0:\n",
    "                                            matching_rows = original_matching_rows.copy()\n",
    "                                            print('Matching Rows Data Frame Updated to Last Successful Filter')\n",
    "\n",
    "                                except ValueError:\n",
    "                                    print(f\"Invalid input for index {selected_index}. Please enter a valid value of the expected data type.\")\n",
    "            else:\n",
    "                possible_values = matching_rows[selected_header].unique()\n",
    "                print(f\"Unique possible values for {selected_header}:\")\n",
    "                for value in possible_values:\n",
    "                    print(value)\n",
    "\n",
    "                filter_value = input(f\"Enter the value to filter by {selected_header}: \")\n",
    "                filtering_criteria.append(f\"Filter Value: {filter_value}\")\n",
    "                \n",
    "                selected_value_type = type(matching_rows[selected_header].iloc[0])\n",
    "                try:\n",
    "                    converted_user_input = selected_value_type(filter_value)\n",
    "                    matching_rows = matching_rows[matching_rows[selected_header] == converted_user_input]\n",
    "\n",
    "                    if len(matching_rows) == 0:\n",
    "                        print(\"No matches found.\")\n",
    "                        matching_rows = original_matching_rows.copy()\n",
    "                    else:\n",
    "                        original_matching_rows = matching_rows.copy()\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Please enter a valid value of the expected data type.\")\n",
    "\n",
    "            continue_input = input(\"Do you want to continue? (y/n): \")\n",
    "            if continue_input.lower() == 'n':\n",
    "                break\n",
    "        else:\n",
    "            print(\"Invalid header number. Please enter a valid number.\")\n",
    "    except ValueError as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        \n",
    "filtering_criteria.append(f\"Matching Rows Count: {len(matching_rows)}\")\n",
    "# Create a set of row indices in matching_rows\n",
    "matching_rows_indices = set(matching_rows.index)\n",
    "\n",
    "# Create a copy of matching_tuples to iterate over while removing rows\n",
    "matching_tuples_copy = matching_tuples.copy()\n",
    "\n",
    "# Iterate over the keys (row indices) in matching_tuples\n",
    "for row_index in matching_tuples_copy.keys():\n",
    "    # Check if the row_index is not in matching_rows_indices\n",
    "    if row_index not in matching_rows_indices:\n",
    "        # Remove the row from matching_tuples\n",
    "        del matching_tuples[row_index]\n",
    "\n",
    "# Now, matching_tuples only contains rows that are both in matching_tuples and matching_rows                 \n",
    "# Display all selections and filtering criteria\n",
    "print(\"\\nAll Selections and Filtering Criteria:\")\n",
    "for criterion in filtering_criteria:\n",
    "    print(criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(matching_rows))\n",
    "for row_index, values in matching_tuples.items():\n",
    "    print('Number of matching tuples:', len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Mouse Genotype   Date Selected Behavior Sham  Session Number  \\\n",
      "3   AD10       D1  62623        Locomotion    n               1   \n",
      "4   AD10       D1  72723        Locomotion    n               2   \n",
      "16  AD11       D1  51223        Locomotion    n               1   \n",
      "17  AD11       D1  72523        Locomotion    n               2   \n",
      "51   P22       D1  62923        Locomotion    n               1   \n",
      "52   P22       D1  72523        Locomotion    n               2   \n",
      "\n",
      "   Fiber Connection                                   Right Turn Array  \\\n",
      "3         Bilateral  [1,0,3,1,1,35,36][1,1,9,2,3,59,62][1,1,6,3,2,6...   \n",
      "4         Bilateral  [1,1,9,1,3,21,24][1,1,9,2,3,30,33][1,1,6,3,2,4...   \n",
      "16        Bilateral  [1,2,18,1,6,121,127][1,1,12,2,4,158,162][1,1,6...   \n",
      "17        Bilateral  [1,0,3,1,1,77,78][1,2,18,2,6,83,89][1,0,3,3,1,...   \n",
      "51        Bilateral  [1,3,27,1,9,23,32][1,1,9,2,3,39,42][1,0,3,3,1,...   \n",
      "52        Bilateral  [1,1,6,1,2,9,11][1,0,3,2,1,21,22][1,1,9,3,3,25...   \n",
      "\n",
      "   Right Turn Array Continue  Total Right Turn Bout Count  ...  \\\n",
      "3                          0                          295  ...   \n",
      "4                          0                          667  ...   \n",
      "16                         0                          823  ...   \n",
      "17                         0                           40  ...   \n",
      "51                         0                         1000  ...   \n",
      "52                         0                          831  ...   \n",
      "\n",
      "    Total Locomotion Bouts in Stim Block  \\\n",
      "3                                    514   \n",
      "4                                    333   \n",
      "16                                   443   \n",
      "17                                    28   \n",
      "51                                   401   \n",
      "52                                   386   \n",
      "\n",
      "    Locomotion Average Duration in Stim Block(seconds)  \\\n",
      "3                                                0.43    \n",
      "4                                                0.41    \n",
      "16                                               0.33    \n",
      "17                                               0.50    \n",
      "51                                               0.26    \n",
      "52                                               0.28    \n",
      "\n",
      "                                     Face Groom Array  \\\n",
      "3   [3,2,21,1,7,96,103][3,1,6,2,2,107,109][3,1,6,3...   \n",
      "4   [3,0,3,1,1,148,149][3,3,27,2,9,288,297][3,0,3,...   \n",
      "16  [3,0,3,1,1,4,5][3,1,6,2,2,12,14][3,0,3,3,1,19,...   \n",
      "17  [3,1,9,1,3,52,55][3,1,6,2,2,570,572][3,0,3,3,1...   \n",
      "51  [3,1,9,1,3,88,91][3,2,15,2,5,235,240][3,4,36,3...   \n",
      "52  [3,1,6,1,2,117,119][3,0,3,2,1,123,124][3,1,9,3...   \n",
      "\n",
      "    Face Groom Array Continue  Total Face Groom Bout Count  \\\n",
      "3                           0                          333   \n",
      "4                           0                          216   \n",
      "16                          0                          887   \n",
      "17                          0                          950   \n",
      "51                          0                         1011   \n",
      "52                          0                         1051   \n",
      "\n",
      "   Total Face Groom Stim Count Average Duration of Face Groom(seconds)  \\\n",
      "3                          497                                   0.464   \n",
      "4                          125                                   0.218   \n",
      "16                         539                                   0.220   \n",
      "17                         980                                   0.339   \n",
      "51                        1045                                   0.341   \n",
      "52                         955                                   0.303   \n",
      "\n",
      "    Total Face Groom Stim in Stim Block  Total Face Groom Bouts in Stim Block  \\\n",
      "3                                    59                                    61   \n",
      "4                                    66                                   110   \n",
      "16                                  254                                   410   \n",
      "17                                  627                                   597   \n",
      "51                                  554                                   503   \n",
      "52                                  473                                   507   \n",
      "\n",
      "    Face Groom Average Duration in Stim Block(seconds)  \n",
      "3                                                0.33   \n",
      "4                                                0.23   \n",
      "16                                               0.22   \n",
      "17                                               0.35   \n",
      "51                                               0.37   \n",
      "52                                               0.31   \n",
      "\n",
      "[6 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "print(matching_rows)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for row_index, values in matching_tuples.items():\n",
    "    print(f\"Row Index: {row_index}, Values: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67]\n",
      "[2.1044776119402986]\n",
      "[67, 346]\n",
      "[2.1044776119402986, 1.8988439306358382]\n",
      "[67, 346, 492]\n",
      "[2.1044776119402986, 1.8988439306358382, 1.7865853658536586]\n",
      "['Right', 'Turn', 'Array']\n",
      "['Right', 'Turn', 'Array']\n",
      "('D1 Right Turn Ipsilateral Right Turn Bout Count Stim', 'D1 Right Turn Ipsilateral Right Turn Duration Stim') ('D1 Right Turn Ipsilateral Right Turn Bout Count Stim', 'D1 Right Turn Ipsilateral Right Turn Duration Stim')\n",
      "['Right', 'Turn', 'Array']\n",
      "[134]\n",
      "[5.097014925373134]\n",
      "[134, 359]\n",
      "[5.097014925373134, 3.604456824512535]\n",
      "[134, 359, 437]\n",
      "[5.097014925373134, 3.604456824512535, 2.6338672768878717]\n",
      "['Locomotion', 'Array']\n",
      "['Locomotion', 'Array']\n",
      "('D1 Right Turn Ipsilateral Locomotion Bout Count Stim', 'D1 Right Turn Ipsilateral Locomotion Duration Stim') ('D1 Right Turn Ipsilateral Locomotion Bout Count Stim', 'D1 Right Turn Ipsilateral Locomotion Duration Stim')\n",
      "['Locomotion', 'Array']\n",
      "[540]\n",
      "[2.738888888888889]\n",
      "[540, 311]\n",
      "[2.738888888888889, 2.7781350482315115]\n",
      "[540, 311, 242]\n",
      "[2.738888888888889, 2.7781350482315115, 3.190082644628099]\n",
      "['Face', 'Groom', 'Array']\n",
      "['Face', 'Groom', 'Array']\n",
      "('D1 Right Turn Ipsilateral Face Groom Bout Count Stim', 'D1 Right Turn Ipsilateral Face Groom Duration Stim') ('D1 Right Turn Ipsilateral Face Groom Bout Count Stim', 'D1 Right Turn Ipsilateral Face Groom Duration Stim')\n",
      "['Face', 'Groom', 'Array']\n"
     ]
    }
   ],
   "source": [
    "# Define a function to determine two different keys based on conditions\n",
    "def determine_key(selected_header, unique_header_3_value, unique_header_4_value,unique_header_0_value,unique_header_10_value, unique_header_1_value,unique_header_6_value):\n",
    "    # Split the selected_header by spaces and enumerate the strings\n",
    "    header_parts = selected_header.split()\n",
    "    print(header_parts)\n",
    "    for i, part in enumerate(header_parts):\n",
    "        if part == \"Array\":\n",
    "            if i >= 2:\n",
    "                new_selected_header = f\"{header_parts[i - 2]} {header_parts[i - 1]}\"\n",
    "            else:\n",
    "                new_selected_header = f\"{header_parts[i - 1]}\"\n",
    "\n",
    "        # Initialize two keys with common components\n",
    "    key_1 = f\"{unique_header_1_value} {unique_header_3_value} {new_selected_header} Bout Count {unique_header_0_value}\"\n",
    "    key_2 = f\"{unique_header_1_value} {unique_header_3_value} {new_selected_header} Duration {unique_header_0_value}\"\n",
    "    # Check if unique_header_4_value is equal to \"y\" for both keys\n",
    "    if unique_header_4_value == \"y\":\n",
    "        key_1 = f\"{unique_header_1_value} {new_selected_header} Bout Count {unique_header_10_value}\"\n",
    "        key_2= f\"{unique_header_1_value} {new_selected_header} Duration {unique_header_10_value}\"\n",
    "    else:\n",
    "        # Check if \"Right Turn\" is in unique_header_3_value for key_1\n",
    "        if \"Right Turn\" in unique_header_3_value:\n",
    "            key_1 = f\"{unique_header_1_value} {unique_header_3_value} {unique_header_6_value} {new_selected_header} Bout Count {unique_header_0_value }\"\n",
    "            key_2=f\"{unique_header_1_value} {unique_header_3_value} {unique_header_6_value} {new_selected_header} Duration {unique_header_0_value }\"\n",
    "\n",
    "    return key_1,key_2\n",
    "\n",
    "# Initialize the averages_dict\n",
    "averages_dict = {}\n",
    "\n",
    "# Load the existing averages_dict from the file if it exists\n",
    "averages_dict_file = \"Practice\"\n",
    "try:\n",
    "    with open(averages_dict_file, 'r') as file:\n",
    "        averages_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    averages_dict = {}\n",
    "\n",
    "# Get a list of unique headers\n",
    "unique_headers = matching_rows.columns.tolist()  # Get all available columns as headers\n",
    "\n",
    "# Uncomment one of the following lines based on your choice:\n",
    "\n",
    "# Option 1: Specify header indices manually\n",
    "header_indices_to_process = [7, 15, 23]\n",
    "\n",
    "# Option 2: Choose a single header using user input\n",
    "#print(\"\\nAvailable headers:\")\n",
    "#for idx, header in enumerate(matching_rows.columns):\n",
    "    #print(f\"{idx}: {header}\")\n",
    "#header_selection = int(input(\"Enter the number of the selected header: \"))\n",
    "#header_indices_to_process = [header_selection] if 'header_selection' in locals() else []\n",
    "\n",
    "# Initialize key1 and key2 with empty strings\n",
    "key1 = \"\"\n",
    "\n",
    "# Iterate over the selected header indices\n",
    "for header_selection in header_indices_to_process:\n",
    "    selected_header = matching_rows.columns[header_selection]\n",
    "\n",
    "    # Filter the DataFrame to select all rows with the selected header\n",
    "    filtered_rows = matching_rows[selected_header]\n",
    "\n",
    "    # Set the values for unique_header_1_value, unique_header_3_value, and unique_header_4_value\n",
    "    unique_header_1_value = matching_rows.iloc[0, 1]\n",
    "    unique_header_3_value = matching_rows.iloc[0, 3]\n",
    "    unique_header_4_value = matching_rows.iloc[0, 4]\n",
    "    unique_header_6_value = matching_rows.iloc[0, 6]\n",
    "    unique_header_0_value = \"Stim\"\n",
    "    unique_header_10_value = \"Sham\"\n",
    "\n",
    "    # Initialize lists to store average stim and post durations for each row\n",
    "    average_boutcount = []\n",
    "    average_duration = []\n",
    "    # Process the filtered data for both \"stim\" and \"post\"\n",
    "    for row_index, row in filtered_rows.items():\n",
    "        # Initialize lists for durations in the current row for both \"bout count\"\n",
    "        boutcount = []\n",
    "        duration=[]\n",
    "\n",
    "        # Remove the additional square brackets at the beginning and end of the row\n",
    "        row = row.strip('[]')\n",
    "\n",
    "        # Split the row into individual tuples using '][' as the delimiter\n",
    "        tuples_list = row.split('][')\n",
    "\n",
    "        # Iterate over the tuples in the list\n",
    "        for tuple_str in tuples_list:\n",
    "            # Remove the square brackets and split the tuple values by comma\n",
    "            values = [float(val) for val in tuple_str.strip('[]').split(',')]\n",
    "\n",
    "            # Now, 'values' contains the individual values as floats\n",
    "            # You can access and manipulate the values based on your conditions\n",
    "            if len(values) >= 7:\n",
    "                value5 = values[5]\n",
    "                value6 = values[6]\n",
    "\n",
    "                # Apply conditions for \"stim\" analysis\n",
    "                if value5 > 3000 and value6 <= 21000:\n",
    "                    boutcount.append(values[3])  # Append the bout value to the stim_boutcount list\n",
    "                    duration.append(values[4])\n",
    "           # Print bout count and duration for the current row\n",
    "       # print(f\"Row {row_index}: Bout Count = {boutcount}, Duration = {duration}\")\n",
    "        # Initialize the variable to store the integer average\n",
    "    \n",
    "        average_boutcount_value = len(boutcount) if boutcount else None\n",
    "        \n",
    "        average_duration_value=(sum(duration)/len(duration)) if duration else None\n",
    "        \n",
    "        # Print the average bout count along with the row index (or any relevant identifier)\n",
    "         # Print bout count and duration for the current row\n",
    "        #print(f\"Row {row_index}: Bout Count = {average_boutcount_value}, Duration = {average_duration_value}\")\n",
    "        # Append the average bout count value for this row to the list\n",
    "        average_boutcount.append(average_boutcount_value)\n",
    "        average_duration.append(average_duration_value)\n",
    "        print(average_boutcount)\n",
    "        print(average_duration)\n",
    "        \n",
    "        # Print the calculated averages for the current row\n",
    "        #print(f\"Row {row_index}: Average Stim Duration: {average_stim_duration}, Average Post Duration: {average_post_duration}\")\n",
    "\n",
    "    # Determine the key for the current row\n",
    "    key1 = determine_key(selected_header, unique_header_3_value, unique_header_4_value, unique_header_0_value, unique_header_10_value, unique_header_1_value, unique_header_6_value)\n",
    "    key2 = determine_key(selected_header, unique_header_3_value, unique_header_4_value, unique_header_0_value, unique_header_10_value, unique_header_1_value, unique_header_6_value)\n",
    "    print(key1,key2)\n",
    "\n",
    "\n",
    "    # Create a dictionary to hold the durations for the current row\n",
    "    boutcount_data = {\n",
    "        'bout count':average_boutcount,\n",
    "        'duration count':average_duration\n",
    "    }\n",
    "    # Print the duration_data dictionary\n",
    "    #print(\"Duration Data:\")\n",
    "    #print(duration_data)\n",
    "\n",
    "    # Determine the keys for the current header\n",
    "    key1, key2 = determine_key(selected_header, unique_header_3_value, unique_header_4_value, unique_header_0_value, unique_header_10_value, unique_header_1_value, unique_header_6_value)\n",
    "\n",
    "    # Use key1 for bout counts and key2 for durations\n",
    "    if key1 not in averages_dict:\n",
    "        averages_dict[key1] = {}\n",
    "    averages_dict[key1] = average_boutcount\n",
    "\n",
    "    if key2 not in averages_dict:\n",
    "        averages_dict[key2] = {}\n",
    "    averages_dict[key2]= average_duration\n",
    "\n",
    "# Save the averages_dict to a file\n",
    "try:\n",
    "    with open(averages_dict_file, 'w') as file:\n",
    "        json.dump(averages_dict, file, separators=(',', ':'))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys without 'Sham' (showing first 3 elements):\n",
      "1. D1 Locomotion Right Turn Bout Count Stim\n",
      "2. D1 Locomotion Locomotion Bout Count Stim\n",
      "3. D1 Locomotion Face Groom Bout Count Stim\n",
      "4. D1 Face Groom Right Turn Bout Count Stim\n",
      "5. D1 Face Groom Locomotion Bout Count Stim\n",
      "6. D1 Face Groom Face Groom Bout Count Stim\n",
      "7. D1 Right Turn Bilateral Right Turn Bout Count Stim\n",
      "8. D1 Right Turn Bilateral Locomotion Bout Count Stim\n",
      "9. D1 Right Turn Bilateral Face Groom Bout Count Stim\n",
      "10. D1 Right Turn Contralateral Right Turn Bout Count Stim\n",
      "11. D1 Right Turn Contralateral Locomotion Bout Count Stim\n",
      "12. D1 Right Turn Contralateral Face Groom Bout Count Stim\n",
      "13. D1 Right Turn Ipsilateral Right Turn Bout Count Stim\n",
      "14. D1 Right Turn Ipsilateral Locomotion Bout Count Stim\n",
      "15. D1 Right Turn Ipsilateral Face Groom Bout Count Stim\n",
      "Enter the number corresponding to Key1: 15\n",
      "You have selected Key1: D1 Right Turn Ipsilateral Face Groom Bout Count Stim\n",
      "D1 Right Turn Ipsilateral Face Groom\n",
      "Auto-filled options for Key2:\n",
      "1. D1 Right Turn Ipsilateral Face Groom\n",
      "Auto-filled Key2 based on matching criteria: D1 Right Turn Ipsilateral Face Groom Duration Stim\n",
      "Bouts: [540, 311, 242]\n",
      "Duration:[2.738888888888889, 2.7781350482315115, 3.190082644628099]\n",
      "Converted Durations: [0.2738888888888889, 0.27781350482315115, 0.31900826446280994]\n",
      "[147.9, 86.4, 77.2]\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the JSON file where you want to store the dictionary\n",
    "p_value_file = \"Bout x duration\"\n",
    "averages_dict_file = \"Practice\"\n",
    "\n",
    "\n",
    "# Initialize the p_value_dict as a global variable or load it from the JSON file if it exists\n",
    "try:\n",
    "    with open(p_value_file, 'r') as file:\n",
    "        p_value_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    p_value_dict = {}\n",
    "\n",
    "# Initialize the averages_dict as a global variable or load it from the JSON file if it exists\n",
    "try:\n",
    "    with open(averages_dict_file, 'r') as file:\n",
    "        averages_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # Handle the file not found error appropriately\n",
    "    averages_dict = {}\n",
    "\n",
    "# Extract keys without 'Sham' and keys with 'Sham' from averages_dict\n",
    "keys_bout = [key for key in averages_dict.keys() if 'Duration' not in key]\n",
    "keys_duration = [key for key in averages_dict.keys() if 'Duration' in key]\n",
    "\n",
    "# Check if there are keys without 'Sham' to use as Key1\n",
    "if keys_bout:\n",
    "    print(\"Available keys without 'Sham' (showing first 3 elements):\")\n",
    "    for idx, key in enumerate(keys_bout, start=1):\n",
    "        # Split the key using space as the separator\n",
    "        key_elements = key.split(' ')\n",
    "        formatted_key = ' '.join(key_elements)\n",
    "\n",
    "        print(f\"{idx}. {formatted_key}\")\n",
    "\n",
    "    # Prompt the user to select Key1\n",
    "    try:\n",
    "        selected_key_idx1 = int(input(\"Enter the number corresponding to Key1: \")) - 1\n",
    "        key1 = keys_bout[selected_key_idx1]\n",
    "        print(f\"You have selected Key1: {key1}\")\n",
    "        words = key1.split(' ')\n",
    "\n",
    "        # Find the index of the word 'bout' in the list of words\n",
    "        try:\n",
    "            index_of_bout = words.index('Bout')\n",
    "        except ValueError:\n",
    "            index_of_bout = len(words)\n",
    "\n",
    "        # Create a new list containing words up until 'bout'\n",
    "        matching_criteria_1 = words[:index_of_bout]\n",
    "\n",
    "        # Join the words back into a string if needed\n",
    "        matching_criteria_1_str = ' '.join(matching_criteria_1)\n",
    "\n",
    "        print(matching_criteria_1_str)\n",
    "\n",
    "        auto_fill_keys = [key for key in keys_duration if matching_criteria_1 == key.split(' ')[:index_of_bout]]\n",
    "\n",
    "        if auto_fill_keys:\n",
    "            print(\"Auto-filled options for Key2:\")\n",
    "            for idx, key in enumerate(auto_fill_keys[:5], start=1):\n",
    "                # Extract the first elements of the key based on index_of_bout\n",
    "                key_elements = key.split(' ')[:index_of_bout]\n",
    "                formatted_key = ' '.join(key_elements)\n",
    "                print(f\"{idx}. {formatted_key}\")\n",
    "\n",
    "            # Automatically set Key2 based on the matching criteria\n",
    "            key2 = auto_fill_keys[0]  # You can adjust this if you have specific logic to choose among them\n",
    "            print(f\"Auto-filled Key2 based on matching criteria: {key2}\")\n",
    "            \n",
    "            # Retrieve the samples associated with the selected keys\n",
    "            bout_samples = averages_dict.get(key1, [])\n",
    "            duration_samples = averages_dict.get(key2, [])\n",
    "            print(f\"Bouts: {bout_samples}\")\n",
    "            print(f\"Duration:{duration_samples}\")\n",
    "            \n",
    "            converted_duration_samples=[]\n",
    "            \n",
    "            for x in duration_samples:\n",
    "                new_duration= x/10\n",
    "                converted_duration_samples.append(new_duration)\n",
    "           \n",
    "            print(f\"Converted Durations: {converted_duration_samples}\")\n",
    "            \n",
    "            time_during_stim = []\n",
    "\n",
    "            for index, (x, y) in enumerate(zip(bout_samples, converted_duration_samples)):\n",
    "                minutes = (x * y)\n",
    "                time_during_stim.append(minutes)\n",
    "\n",
    "            print(time_during_stim)\n",
    "            \n",
    "            # Convert the tuple keys to strings before updating the p_value_dict\n",
    "            str_key1 = ' '.join(matching_criteria_1)\n",
    "\n",
    "            # Update the p_value_dict with the p-values using the selected keys\n",
    "    \n",
    "            p_value_dict[str_key1] = {\n",
    "                'time in seconds':time_during_stim,\n",
    "            }\n",
    "\n",
    "            # Save the updated p_value_dict back to the JSON file\n",
    "            with open(p_value_file, 'w') as file:\n",
    "                json.dump(p_value_dict, file)\n",
    "        else:\n",
    "            print(\"No auto-filled options available based on Key1.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input for Key1. Please enter a valid number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════════════════════════════╤════════════════════════════╤════════════════════════════╤════════════════════════════╤════════════════════════════╤════════════════════════════╤════════════════════════════╤══════════════════════════════════════╤══════════════════════════════════════╤══════════════════════════════════════╤══════════════════════════════════════════╤══════════════════════════════════════════╤══════════════════════════════════════════╤════════════════════════════════════════╤════════════════════════════════════════╤════════════════════════════════════════╕\n",
      "│                                       │  d1 face groom face groom  │  d1 face groom locomotion  │  d1 face groom right turn  │  d1 locomotion face groom  │  d1 locomotion locomotion  │  d1 locomotion right turn  │  d1 right turn bilateral face groom  │  d1 right turn bilateral locomotion  │  d1 right turn bilateral right turn  │  d1 right turn contralateral face groom  │  d1 right turn contralateral locomotion  │  d1 right turn contralateral right turn  │  d1 right turn ipsilateral face groom  │  d1 right turn ipsilateral locomotion  │  d1 right turn ipsilateral right turn  │\n",
      "╞═══════════════════════════════════════╪════════════════════════════╪════════════════════════════╪════════════════════════════╪════════════════════════════╪════════════════════════════╪════════════════════════════╪══════════════════════════════════════╪══════════════════════════════════════╪══════════════════════════════════════╪══════════════════════════════════════════╪══════════════════════════════════════════╪══════════════════════════════════════════╪════════════════════════════════════════╪════════════════════════════════════════╪════════════════════════════════════════╡\n",
      "│        d face groom face groom        │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│        d face groom locomotion        │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│        d face groom right turn        │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│        d locomotion face groom        │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│        d locomotion locomotion        │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│        d locomotion right turn        │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│   d right turn bilateral face groom   │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│   d right turn bilateral locomotion   │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│   d right turn bilateral right turn   │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│ d right turn contralateral face groom │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│ d right turn contralateral locomotion │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│ d right turn contralateral right turn │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│  d right turn ipsilateral face groom  │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│  d right turn ipsilateral locomotion  │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "├───────────────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼──────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┼────────────────────────────────────────┤\n",
      "│  d right turn ipsilateral right turn  │                            │                            │                            │                            │                            │                            │                                      │                                      │                                      │                                          │                                          │                                          │                                        │                                        │                                        │\n",
      "╘═══════════════════════════════════════╧════════════════════════════╧════════════════════════════╧════════════════════════════╧════════════════════════════╧════════════════════════════╧════════════════════════════╧══════════════════════════════════════╧══════════════════════════════════════╧══════════════════════════════════════╧══════════════════════════════════════════╧══════════════════════════════════════════╧══════════════════════════════════════════╧════════════════════════════════════════╧════════════════════════════════════════╧════════════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Read data from the JSON file\n",
    "with open('Bout x duration', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Create lists to hold row and column labels\n",
    "row_labels = sorted(set(re.sub(r'[^A-Za-z ]+', '', key).strip().lower() for key in data.keys()))\n",
    "column_labels = sorted(set(event.lower() for event in data.keys()))\n",
    "\n",
    "# Create a table to display the time in seconds\n",
    "table_data = []\n",
    "\n",
    "# Iterate through the data and populate the table\n",
    "for row_label in row_labels:\n",
    "    row = [row_label]\n",
    "    for column_label in column_labels:\n",
    "        key = f\"{row_label} {column_label}\"\n",
    "        time_in_seconds = data.get(key, {}).get(\"time in seconds\", [])\n",
    "        time_str = ', '.join([f'{time:.2f}' for time in time_in_seconds])\n",
    "        row.append(time_str)\n",
    "    table_data.append(row)\n",
    "\n",
    "# Print the table\n",
    "table_headers = [\"\"] + column_labels\n",
    "table = tabulate(table_data, headers=table_headers, tablefmt=\"fancy_grid\", numalign=\"center\", stralign=\"center\")\n",
    "\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Columns In DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available headers:\n",
      "0: Mouse\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [286]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, header \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(matching_rows\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     header_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter the number of the selected header: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     53\u001b[0m header_indices_to_process \u001b[38;5;241m=\u001b[39m [header_selection] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader_selection\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Iterate over the selected header indices\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Define a function to determine the key based on conditions\n",
    "def determine_key(selected_header, unique_header_3_value, unique_header_4_value):\n",
    "    # Split the selected_header by spaces and enumerate the strings\n",
    "    header_parts = selected_header.split()\n",
    "    for i, part in enumerate(header_parts):\n",
    "        if part == \"Stim\":\n",
    "            block=\"Stim Bock\"\n",
    "        if part == \"Bouts\":\n",
    "            # Check the string before \"Bouts\"\n",
    "            if i > 0 and header_parts[i - 1] == \"Turn\":\n",
    "                new_selected_header = 'Right Turn Bouts'\n",
    "            elif i > 0 and header_parts[i - 1] == \"Groom\":\n",
    "                new_selected_header = 'Face Groom Bouts'\n",
    "            else:\n",
    "                new_selected_header = f\"{header_parts[i - 1]} Bouts\"\n",
    "\n",
    "    # Check if unique header 3 contains \"Right Turn\"\n",
    "    if \"Right Turn\" in unique_header_3_value:\n",
    "        return [unique_header_1_value, unique_header_3_value, unique_header_6_value, new_selected_header,block]\n",
    "\n",
    "    # Check if unique header 4 has value \"y\"\n",
    "    if unique_header_4_value == \"y\":\n",
    "        return [unique_header_1_value, new_selected_header, matching_rows.columns[4]]\n",
    "\n",
    "    # Default key order\n",
    "    return [unique_header_1_value, unique_header_3_value, new_selected_header,block]\n",
    "\n",
    "# Initialize the averages_dict\n",
    "averages_dict = {}\n",
    "\n",
    "# Load the existing averages_dict from the file if it exists\n",
    "averages_dict_file = \"Stim_Sham_Inner_Session_dict_file\"\n",
    "try:\n",
    "    with open(averages_dict_file, 'r') as file:\n",
    "        averages_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    averages_dict = {}\n",
    "\n",
    "# Get a list of unique headers\n",
    "unique_headers = matching_rows.columns.tolist()  # Get all available columns as headers\n",
    "\n",
    "# Uncomment one of the following lines based on your choice:\n",
    "\n",
    "# Option 1: Specify header indices manually\n",
    "#header_indices_to_process = [13, 21, 29]\n",
    "\n",
    "# Option 2: Choose a single header using user input\n",
    "print(\"\\nAvailable headers:\")\n",
    "for idx, header in enumerate(matching_rows.columns):\n",
    "    print(f\"{idx}: {header}\")\n",
    "    header_selection = int(input(\"Enter the number of the selected header: \"))\n",
    "header_indices_to_process = [header_selection] if 'header_selection' in locals() else []\n",
    "\n",
    "# Iterate over the selected header indices\n",
    "for header_selection in header_indices_to_process:\n",
    "    selected_header = matching_rows.columns[header_selection]\n",
    "\n",
    "    # Filter the DataFrame to select all rows with the selected header\n",
    "    filtered_rows = matching_rows[selected_header]\n",
    "\n",
    "    # Set the values for unique_header_1_value, unique_header_3_value, and unique_header_4_value\n",
    "    unique_header_1_value = matching_rows.iloc[0, 1]\n",
    "    unique_header_3_value = matching_rows.iloc[0, 3]\n",
    "    unique_header_4_value = matching_rows.iloc[0, 4]\n",
    "    unique_header_6_value = matching_rows.iloc[0, 6]\n",
    "\n",
    "    # Use the determine_key function to determine the key based on conditions\n",
    "    key_order = determine_key(selected_header, unique_header_3_value, unique_header_4_value)\n",
    "\n",
    "    # Initialize a list for bout_count\n",
    "    bout_count = []\n",
    "\n",
    "    # Process the filtered data\n",
    "    for value in filtered_rows:\n",
    "        # Append value to the bout_count list\n",
    "        bout_count.append(value)\n",
    "\n",
    "    # Determine the key based on the selected_header and other conditions\n",
    "    key = \" \".join(key_order)\n",
    "    print(key)\n",
    "    # Append the bout_count to the list associated with the key\n",
    "    #if key in averages_dict:\n",
    "        #averages_dict[key].extend(bout_count)\n",
    "    #else:\n",
    "        #averages_dict[key] = bout_count\n",
    "\n",
    "# Save the updated averages_dict to the file without double square brackets\n",
    "#with open(averages_dict_file, 'w') as file:\n",
    "    #json.dump(averages_dict, file, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys without 'Sham' (showing first 3 elements):\n",
      "1. D1 Right Turn Contralateral Right\n",
      "2. D1 Right Turn Contralateral Locomotion\n",
      "3. D1 Right Turn Contralateral Face\n",
      "4. D1 Right Turn Bilateral Right\n",
      "5. D1 Right Turn Bilateral Locomotion\n",
      "6. D1 Right Turn Bilateral Face\n",
      "7. D1 Right Turn Ipsilateral Right\n",
      "8. D1 Right Turn Ipsilateral Locomotion\n",
      "9. D1 Right Turn Ipsilateral Face\n",
      "10. D1 Locomotion Right\n",
      "11. D1 Locomotion Locomotion\n",
      "12. D1 Locomotion Face\n",
      "13. D1 Face Groom Right\n",
      "14. D1 Face Groom Locomotion\n",
      "15. D1 Face Groom Face\n",
      "Enter the number corresponding to Key1: 15\n",
      "You have selected Key1: D1 Face Groom Face Groom Bouts Stim Bock\n",
      "['D1', 'Face', 'Groom', 'Face', 'Groom', 'Bouts', 'Stim', 'Bock']\n",
      "Auto-filled options for Key2:\n",
      "1. D1 Face Groom Bouts\n",
      "Auto-filled Key2 based on matching criteria: D1 Face Groom Bouts Sham\n",
      "T-Test Results:\n",
      "t-statistic (T-Test): -0.3565206633248731\n",
      "p-value (T-Test): 0.7248472551203001\n",
      "Welch's T-Test Results:\n",
      "t-statistic (Welch's T-Test): -0.26799063962256714\n",
      "p-value (Welch's T-Test): 0.803582978406229\n",
      "Bootstrapping Results:\n",
      "Bootstrap p-value (Bootstrapping): 0.7769223077692231\n",
      "There is no significant difference between the means of D1 Face Groom Face Groom Bouts Stim Bock and D1 Face Groom Bouts Sham (T-Test).\n",
      "There is no significant difference between the distributions of D1 Face Groom Face Groom Bouts Stim Bock and D1 Face Groom Bouts Sham (Bootstrapping).\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the JSON file where you want to store the dictionary\n",
    "p_value_file = \"p_value_inner_trial_bout.json\"\n",
    "averages_dict_file = \"Stim_Sham_Inner_Session_dict_file\"\n",
    "\n",
    "# Initialize the p_value_dict as a global variable or load it from the JSON file if it exists\n",
    "try:\n",
    "    with open(p_value_file, 'r') as file:\n",
    "        p_value_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    p_value_dict = {}\n",
    "\n",
    "# Initialize the averages_dict as a global variable or load it from the JSON file if it exists\n",
    "try:\n",
    "    with open(averages_dict_file, 'r') as file:\n",
    "        averages_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # Handle the file not found error appropriately\n",
    "    averages_dict = {}\n",
    "\n",
    "# Extract keys without 'Sham' and keys with 'Sham' from averages_dict\n",
    "keys_without_sham = [key for key in averages_dict.keys() if 'Sham' not in key]\n",
    "keys_with_sham = [key for key in averages_dict.keys() if 'Sham' in key]\n",
    "\n",
    "# Check if there are keys without 'Sham' to use as Key1\n",
    "if keys_without_sham:\n",
    "    print(\"Available keys without 'Sham' (showing first 3 elements):\")\n",
    "    for idx, key in enumerate(keys_without_sham, start=1):\n",
    "        # Split the key using space as the separator\n",
    "        key_elements = key.split(' ')\n",
    "\n",
    "        # Check if element 2 is \"Right\" and combine it with elements 3 and 4\n",
    "        if len(key_elements) >= 4 and key_elements[1] == 'Right':\n",
    "            formatted_key = ' '.join([key_elements[0], key_elements[1] + ' ' + key_elements[2] + ' ' + key_elements[3], key_elements[4]])\n",
    "        elif len(key_elements) >= 3 and key_elements[1] == 'Face':\n",
    "            formatted_key = ' '.join([key_elements[0], key_elements[1] + ' ' + key_elements[2], key_elements[3]])\n",
    "        else:\n",
    "            formatted_key = ' '.join(key_elements[:3])\n",
    "\n",
    "        print(f\"{idx}. {formatted_key}\")\n",
    "\n",
    "    # Prompt the user to select Key1\n",
    "    try:\n",
    "        selected_key_idx1 = int(input(\"Enter the number corresponding to Key1: \")) - 1\n",
    "        key1 = keys_without_sham[selected_key_idx1]\n",
    "        print(f\"You have selected Key1: {key1}\")\n",
    "\n",
    "        # Auto-fill Key2 based on the matching criteria\n",
    "        matching_criteria_1 = key1.split(' ')\n",
    "        print(matching_criteria_1)\n",
    "        if matching_criteria_1[1] == 'Right':\n",
    "            matching_criteria = [matching_criteria_1[0], matching_criteria_1[4]]\n",
    "            auto_fill_keys = [key for key in keys_with_sham if matching_criteria == [key.split(' ')[0], key.split(' ')[1]]]\n",
    "        elif matching_criteria_1[1] == 'Face':\n",
    "            matching_criteria = [matching_criteria_1[0], matching_criteria_1[3]]\n",
    "            auto_fill_keys = [key for key in keys_with_sham if matching_criteria == [key.split(' ')[0], key.split(' ')[1]]]\n",
    "        else:\n",
    "            matching_criteria = [matching_criteria_1[0], matching_criteria_1[2]]\n",
    "            auto_fill_keys = [key for key in keys_with_sham if matching_criteria == [key.split(' ')[0], key.split(' ')[1]]]\n",
    "\n",
    "        if auto_fill_keys:\n",
    "            print(\"Auto-filled options for Key2:\")\n",
    "            for idx, key in enumerate(auto_fill_keys[:5], start=1):\n",
    "                # Extract the first three elements of the key\n",
    "                key_elements = key.split(' ')[:4]\n",
    "                formatted_key = ' '.join(key_elements)\n",
    "                print(f\"{idx}. {formatted_key}\")\n",
    "\n",
    "            # Automatically set Key2 based on the matching criteria\n",
    "            key2 = auto_fill_keys[0]  # You can adjust this if you have specific logic to choose among them\n",
    "            print(f\"Auto-filled Key2 based on matching criteria: {key2}\")\n",
    "\n",
    "            # Retrieve the samples associated with the selected keys\n",
    "            samples1 = averages_dict.get(key1, [])\n",
    "            samples2 = averages_dict.get(key2, [])\n",
    "\n",
    "            # Perform bootstrapping with 10,000 iterations\n",
    "            num_bootstrap_samples = 10000\n",
    "            bootstrap_mean_differences = []\n",
    "\n",
    "            # Perform bootstrapping with 'num_bootstrap_samples' iterations\n",
    "            for _ in range(num_bootstrap_samples):\n",
    "                # Generate bootstrap samples by resampling with replacement\n",
    "                bootstrap_samples1 = np.random.choice(samples1, len(samples1), replace=True)\n",
    "                bootstrap_samples2 = np.random.choice(samples2, len(samples2), replace=True)\n",
    "\n",
    "                # Calculate the mean difference between bootstrap_samples1 and bootstrap_samples2\n",
    "                bootstrap_mean_difference = np.mean(bootstrap_samples1) - np.mean(bootstrap_samples2)\n",
    "\n",
    "                # Append the mean difference to the list\n",
    "                bootstrap_mean_differences.append(bootstrap_mean_difference)\n",
    "\n",
    "            # Calculate the observed difference in means between the original samples\n",
    "            observed_diff = np.mean(samples1) - np.mean(samples2)\n",
    "\n",
    "            # Calculate the p-value for the bootstrap test\n",
    "            bootstrap_p_value = (np.sum(np.abs(bootstrap_mean_differences) >= np.abs(observed_diff)) + 1) / (num_bootstrap_samples + 1)\n",
    "\n",
    "            # Perform a t-test on the original samples\n",
    "            t_statistic_ttest, p_value_ttest = stats.ttest_ind(samples1, samples2)\n",
    "\n",
    "            # Perform Welch's t-test\n",
    "            t_statistic_welch, p_value_welch = stats.ttest_ind(samples1, samples2, equal_var=False)\n",
    "\n",
    "            # Print the t-test results\n",
    "            print(\"T-Test Results:\")\n",
    "            print(\"t-statistic (T-Test):\", t_statistic_ttest)\n",
    "            print(\"p-value (T-Test):\", p_value_ttest)\n",
    "\n",
    "            # Print Welch's T-Test Results\n",
    "            print(\"Welch's T-Test Results:\")\n",
    "            print(\"t-statistic (Welch's T-Test):\", t_statistic_welch)\n",
    "            print(\"p-value (Welch's T-Test):\", p_value_welch)\n",
    "\n",
    "            # Print the bootstrapping results\n",
    "            print(\"Bootstrapping Results:\")\n",
    "            print(\"Bootstrap p-value (Bootstrapping):\", bootstrap_p_value)\n",
    "\n",
    "            # Determine whether the t-test results are statistically significant (typically p < 0.05)\n",
    "            if round(p_value_ttest, 2) <= 0.05:\n",
    "                print(f\"The means of {key1} and {key2} are statistically different (T-Test).\")\n",
    "            else:\n",
    "                print(f\"There is no significant difference between the means of {key1} and {key2} (T-Test).\")\n",
    "\n",
    "            # Determine whether the Mann-Whitney U test results are statistically significant (typically p < 0.05)\n",
    "            if round(bootstrap_p_value, 2) <= 0.05:\n",
    "                print(f\"The distributions of {key1} and {key2} are statistically different (Bootstrapping).\")\n",
    "            else:\n",
    "                print(f\"There is no significant difference between the distributions of {key1} and {key2} (Bootstrapping).\")\n",
    "\n",
    "            # Convert the tuple keys to strings before updating the p_value_dict\n",
    "            str_key1 = ', '.join(matching_criteria_1)\n",
    "\n",
    "            # Update the p_value_dict with the p-values using the selected keys\n",
    "            p_value_dict[str_key1] = {\n",
    "                'p_value_ttest': round(p_value_ttest, 2),\n",
    "                'p_value_welchsttest': round(p_value_welch, 2),\n",
    "                'bootstrap_p_value': round(bootstrap_p_value, 2)\n",
    "            }\n",
    "\n",
    "            # Save the updated p_value_dict back to the JSON file\n",
    "            with open(p_value_file, 'w') as file:\n",
    "                json.dump(p_value_dict, file)\n",
    "        else:\n",
    "            print(\"No auto-filled options available based on Key1.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input for Key1. Please enter a valid number.\")\n",
    "else:\n",
    "    print(\"No keys without 'Sham' available in the dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Right', 'Turn', 'Array']\n",
      "D1 Locomotion Right Turn Duration Stim\n",
      "D1 Locomotion Right Turn Duration Post\n",
      "['Locomotion', 'Array']\n",
      "D1 Locomotion Locomotion Duration Stim\n",
      "D1 Locomotion Locomotion Duration Post\n",
      "['Face', 'Groom', 'Array']\n",
      "D1 Locomotion Face Groom Duration Stim\n",
      "D1 Locomotion Face Groom Duration Post\n"
     ]
    }
   ],
   "source": [
    "# Define a function to determine two different keys based on conditions\n",
    "def determine_key(selected_header, unique_header_3_value, unique_header_4_value,unique_header_0_value,unique_header_10_value, unique_header_1_value,unique_header_6_value):\n",
    "    # Split the selected_header by spaces and enumerate the strings\n",
    "    header_parts = selected_header.split()\n",
    "    print(header_parts)\n",
    "    for i, part in enumerate(header_parts):\n",
    "        if part == \"Array\":\n",
    "            if i >= 2:\n",
    "                new_selected_header = f\"{header_parts[i - 2]} {header_parts[i - 1]} Duration\"\n",
    "            else:\n",
    "                new_selected_header = f\"{header_parts[i - 1]} Duration\"\n",
    "\n",
    "        # Initialize two keys with common components\n",
    "    key_1 = f\"{unique_header_1_value} {unique_header_3_value} {new_selected_header} Stim\"\n",
    "    key_2 = f\"{unique_header_1_value} {unique_header_3_value} {new_selected_header} Post\"\n",
    "\n",
    "    # Check if unique_header_4_value is equal to \"y\" for both keys\n",
    "    if unique_header_4_value == \"y\":\n",
    "        key_1 = f\"{unique_header_1_value} {new_selected_header} Stim Sham\"\n",
    "        key_2 = f\"{unique_header_1_value} {new_selected_header} Post Sham\"\n",
    "    else:\n",
    "        # Check if \"Right Turn\" is in unique_header_3_value for key_1\n",
    "        if \"Right Turn\" in unique_header_3_value:\n",
    "            key_1 = f\"{unique_header_1_value} {unique_header_3_value} {unique_header_6_value} {new_selected_header} Stim\"\n",
    "\n",
    "        # Check if \"Right Turn\" is in unique_header_3_value for key_2\n",
    "        if \"Right Turn\" in unique_header_3_value:\n",
    "            key_2 = f\"{unique_header_1_value} {unique_header_3_value} {unique_header_6_value} {new_selected_header} Post\"\n",
    "\n",
    "    return key_1, key_2\n",
    "\n",
    "# Initialize the averages_dict\n",
    "averages_dict = {}\n",
    "\n",
    "# Load the existing averages_dict from the file if it exists\n",
    "averages_dict_file = \"Duration practice_dict_file\"\n",
    "try:\n",
    "    with open(averages_dict_file, 'r') as file:\n",
    "        averages_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    averages_dict = {}\n",
    "\n",
    "# Get a list of unique headers\n",
    "unique_headers = matching_rows.columns.tolist()  # Get all available columns as headers\n",
    "\n",
    "# Uncomment one of the following lines based on your choice:\n",
    "\n",
    "# Option 1: Specify header indices manually\n",
    "header_indices_to_process = [7, 15, 23]\n",
    "\n",
    "# Option 2: Choose a single header using user input\n",
    "#print(\"\\nAvailable headers:\")\n",
    "#for idx, header in enumerate(matching_rows.columns):\n",
    "    #print(f\"{idx}: {header}\")\n",
    "#header_selection = int(input(\"Enter the number of the selected header: \"))\n",
    "#header_indices_to_process = [header_selection] if 'header_selection' in locals() else []\n",
    "\n",
    "# Initialize key1 and key2 with empty strings\n",
    "key1 = \"\"\n",
    "key2 = \"\"\n",
    "\n",
    "# Iterate over the selected header indices\n",
    "for header_selection in header_indices_to_process:\n",
    "    selected_header = matching_rows.columns[header_selection]\n",
    "\n",
    "    # Filter the DataFrame to select all rows with the selected header\n",
    "    filtered_rows = matching_rows[selected_header]\n",
    "\n",
    "    # Set the values for unique_header_1_value, unique_header_3_value, and unique_header_4_value\n",
    "    unique_header_1_value = matching_rows.iloc[0, 1]\n",
    "    unique_header_3_value = matching_rows.iloc[0, 3]\n",
    "    unique_header_4_value = matching_rows.iloc[0, 4]\n",
    "    unique_header_6_value = matching_rows.iloc[0, 6]\n",
    "    unique_header_0_value = \"Stim\"\n",
    "    unique_header_10_value = \"Post\"\n",
    "\n",
    "    # Initialize lists to store average stim and post durations for each row\n",
    "    average_stim_durations = []\n",
    "    average_post_durations = []\n",
    "\n",
    "    # Process the filtered data for both \"stim\" and \"post\"\n",
    "    for row_index, row in filtered_rows.items():\n",
    "        # Initialize lists for durations in\n",
    "        # the current row for both \"stim\" and \"post\"\n",
    "        stim_durations = []\n",
    "        post_durations = []\n",
    "\n",
    "        # Remove the additional square brackets at the beginning and end of the row\n",
    "        row = row.strip('[]')\n",
    "\n",
    "        # Split the row into individual tuples using '][' as the delimiter\n",
    "        tuples_list = row.split('][')\n",
    "\n",
    "        # Iterate over the tuples in the list\n",
    "        for tuple_str in tuples_list:\n",
    "            # Remove the square brackets and split the tuple values by comma\n",
    "            values = [float(val) for val in tuple_str.strip('[]').split(',')]\n",
    "\n",
    "            # Now, 'values' contains the individual values as floats\n",
    "            # You can access and manipulate the values based on your conditions\n",
    "            if len(values) >= 7:\n",
    "                value5 = values[5]\n",
    "                value6 = values[6]\n",
    "\n",
    "                # Apply conditions for \"stim\" analysis\n",
    "                if value5 > 3000 and value6 <= 21000:\n",
    "                    stim_durations.append(values[4])  # Append the duration value to the stim_durations list\n",
    "                # Apply conditions for \"post\" analysis\n",
    "                if value5 > 21000 and value6 <= 39000:\n",
    "                    post_durations.append(values[4])\n",
    "\n",
    "        # Calculate the average \"stim\" and \"post\" durations for the current row\n",
    "        average_stim_duration = sum(stim_durations) / len(stim_durations) if stim_durations else None\n",
    "        average_post_duration = sum(post_durations) / len(post_durations) if post_durations else None\n",
    "\n",
    "        # Append the average durations to the respective lists\n",
    "        average_stim_durations.append(average_stim_duration)\n",
    "        average_post_durations.append(average_post_duration)\n",
    "\n",
    "        # Print the calculated averages for the current row\n",
    "        #print(f\"Row {row_index}: Average Stim Duration: {average_stim_duration}, Average Post Duration: {average_post_duration}\")\n",
    "\n",
    "    # Determine the key for the current row\n",
    "    key1, key2 = determine_key(selected_header, unique_header_3_value, unique_header_4_value, unique_header_0_value, unique_header_10_value, unique_header_1_value, unique_header_6_value)\n",
    "    print(key1)\n",
    "    print(key2)\n",
    "\n",
    "    # Create a dictionary to hold the durations for the current row\n",
    "    duration_data = {\n",
    "        'stim_duration': average_stim_durations,\n",
    "        'post_duration': average_post_durations\n",
    "    }\n",
    "    # Print the duration_data dictionary\n",
    "    #print(\"Duration Data:\")\n",
    "    #print(duration_data)\n",
    "\n",
    "    # Use key1 and key2 when storing values in the averages_dict\n",
    "    if \"Stim\" in key1:\n",
    "        # Check if 'Stim' key exists in averages_dict, create if not\n",
    "        if 'Stim' not in averages_dict:\n",
    "            averages_dict['Stim'] = {}\n",
    "        averages_dict['Stim'][key1] = duration_data['stim_duration']\n",
    "\n",
    "    if \"Post\" in key2:\n",
    "        # Check if 'Post' key exists in averages_dict, create if not\n",
    "        if 'Post' not in averages_dict:\n",
    "            averages_dict['Post'] = {}\n",
    "        averages_dict['Post'][key2] = duration_data['post_duration']\n",
    "        \n",
    "    # Print the Averages Dictionary\n",
    "    #print(\"Averages Dictionary:\")\n",
    "    #print(averages_dict)\n",
    "try:\n",
    "    with open(averages_dict_file, 'w') as file:\n",
    "        json.dump(averages_dict, file, separators=(',', ':'))\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available 'Post' keys without 'Sham' (showing first 3 elements):\n",
      "1. D1 Locomotion Right\n",
      "2. D1 Locomotion Locomotion\n",
      "3. D1 Locomotion Face\n",
      "Enter the number corresponding to Key1: 1\n",
      "You have selected Key1: D1 Locomotion Right Turn Duration Post\n",
      "Auto-filled options for Key2:\n",
      "1. D1 Locomotion Right Turn\n",
      "Auto-filled Key2 based on matching criteria: D1 Locomotion Right Turn Duration Stim\n",
      "T-Test Results:\n",
      "t-statistic: -0.11970852921398406\n",
      "p-value (T-Test): 0.9070848800908163\n",
      "Bootstrapping Results:\n",
      "Bootstrap p-value: 0.8958104189581042\n",
      "There is no significant difference between the means of D1 Locomotion Right Turn Duration Post and D1 Locomotion Right Turn Duration Stim.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the JSON file where you want to store the dictionary\n",
    "p_value_file = \"p_value_inner_trial.json\"\n",
    "averages_dict_file = \"Duration practice_dict_file\"\n",
    "\n",
    "# Load data from the JSON file\n",
    "try:\n",
    "    with open(averages_dict_file, 'r') as file:\n",
    "        data_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    data_dict = {}\n",
    "\n",
    "# Initialize the p_value_dict as a global variable or load it from the JSON file if it exists\n",
    "try:\n",
    "    with open(p_value_file, 'r') as file:\n",
    "        p_value_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    p_value_dict = {}\n",
    "\n",
    "# Initialize the lists for 'Post' and 'Stim' keys\n",
    "keys_post = []\n",
    "keys_stim = []\n",
    "\n",
    "# Iterate through the sub-dictionaries 'pos' and 'stim'\n",
    "for sub_dict_name in ['Post', 'Stim']:\n",
    "    sub_dict = data_dict.get(sub_dict_name, {})  # Get the sub-dictionary\n",
    "\n",
    "    # Filter 'Post' keys and 'Stim' keys without 'Sham'\n",
    "    post_keys = [key for key in sub_dict.keys() if 'Post' in key and 'Sham' not in key]\n",
    "    stim_keys = [key for key in sub_dict.keys() if 'Stim' in key and 'Sham' not in key]\n",
    "\n",
    "    # Extend the main lists with the filtered keys\n",
    "    keys_post.extend(post_keys)\n",
    "    keys_stim.extend(stim_keys)\n",
    "\n",
    "# Check if there are 'Post' keys without 'Sham' to use as Key1\n",
    "if keys_post:\n",
    "    print(\"Available 'Post' keys without 'Sham' (showing first 3 elements):\")\n",
    "    for idx, key in enumerate(keys_post, start=1):\n",
    "        # Split the key using space as the separator\n",
    "        key_elements = key.split(' ')\n",
    "\n",
    "        # Check if element 2 is \"Right\" and combine it with elements 3 and 4\n",
    "        if len(key_elements) >= 4 and key_elements[1] == 'Right':\n",
    "            formatted_key = ' '.join([key_elements[0], key_elements[1] + ' ' + key_elements[2] + ' ' + key_elements[3],key_elements[4]])\n",
    "        elif len(key_elements)>= 3 and key_elements[1]== 'Face':\n",
    "            formatted_key = ' '.join([key_elements[0], key_elements[1] + ' ' + key_elements[2],key_elements[3]])\n",
    "        else:\n",
    "            formatted_key = ' '.join(key_elements[:3])\n",
    "\n",
    "        print(f\"{idx}. {formatted_key}\")\n",
    "\n",
    "    # Prompt the user to select Key1\n",
    "    try:\n",
    "        selected_key_idx1 = int(input(\"Enter the number corresponding to Key1: \")) - 1\n",
    "        key1 = keys_post[selected_key_idx1]\n",
    "        print(f\"You have selected Key1: {key1}\")\n",
    "\n",
    "     # Auto-fill Key2 based on the matching criteria\n",
    "        matching_criteria_1 = key1.split(' ')\n",
    "        if matching_criteria_1[1] =='Locomotion' and matching_criteria_1[2] =='Locomotion':\n",
    "             matching_criteria = [matching_criteria_1[0], matching_criteria_1[1], matching_criteria_1[2]]\n",
    "        else:\n",
    "            matching_criteria = [matching_criteria_1[0], matching_criteria_1[1], matching_criteria_1[2], matching_criteria_1[3], matching_criteria_1[4]]\n",
    "\n",
    "        # Check if matching_criteria_1[1] and matching_criteria_1[2] are 'Locomotion'\n",
    "        if matching_criteria_1[1] == 'Locomotion' and matching_criteria_1[2] == 'Locomotion':\n",
    "            auto_fill_keys = [key for key in keys_stim if matching_criteria == key.split(' ')[:3]]\n",
    "        else:\n",
    "            auto_fill_keys = [key for key in keys_stim if matching_criteria == key.split(' ')[:5]]\n",
    "\n",
    "        if auto_fill_keys:\n",
    "            print(\"Auto-filled options for Key2:\")\n",
    "            for idx, key in enumerate(auto_fill_keys[:5], start=1):\n",
    "                # Extract the first three elements of the key\n",
    "                key_elements = key.split(' ')[:4]\n",
    "                formatted_key = ' '.join(key_elements)\n",
    "                print(f\"{idx}. {formatted_key}\")\n",
    "\n",
    "            # Automatically set Key2 based on the matching criteria\n",
    "            key2 = auto_fill_keys[0]  # You can adjust this if you have specific logic to choose among them\n",
    "            print(f\"Auto-filled Key2 based on matching criteria: {key2}\")\n",
    "\n",
    "            # Retrieve the samples associated with the selected keys\n",
    "            samples1 = data_dict['Post'].get(key1, [])\n",
    "            samples2 = data_dict['Stim'].get(key2, [])\n",
    "\n",
    "            # Perform bootstrapping with 10000 iterations\n",
    "            num_bootstrap_samples = 10000\n",
    "            bootstrap_mean_differences = []\n",
    "\n",
    "            # Perform bootstrapping with 'num_bootstrap_samples' iterations\n",
    "            for _ in range(num_bootstrap_samples):\n",
    "                # Generate bootstrap samples by resampling with replacement\n",
    "                bootstrap_samples1 = np.random.choice(samples1, len(samples1), replace=True)\n",
    "                bootstrap_samples2 = np.random.choice(samples2, len(samples2), replace=True)\n",
    "\n",
    "                # Calculate the mean difference between bootstrap_samples1 and bootstrap_samples2\n",
    "                bootstrap_mean_difference = np.mean(bootstrap_samples1) - np.mean(bootstrap_samples2)\n",
    "\n",
    "                # Append the mean difference to the list\n",
    "                bootstrap_mean_differences.append(bootstrap_mean_difference)\n",
    "\n",
    "            # Calculate the observed difference in means between the original samples\n",
    "            observed_diff = np.mean(samples1) - np.mean(samples2)\n",
    "\n",
    "            # Calculate the p-value for the bootstrap test\n",
    "            bootstrap_p_value = (np.sum(np.abs(bootstrap_mean_differences) >= np.abs(observed_diff)) + 1) / (\n",
    "                num_bootstrap_samples + 1)\n",
    "\n",
    "            # Perform a t-test on the original samples\n",
    "            t_statistic, p_value = stats.ttest_ind(samples1, samples2)\n",
    "\n",
    "            # Print the t-test results\n",
    "            print(\"T-Test Results:\")\n",
    "            print(\"t-statistic:\", t_statistic)\n",
    "            print(\"p-value (T-Test):\", p_value)\n",
    "\n",
    "            # Print the bootstrapping results\n",
    "            print(\"Bootstrapping Results:\")\n",
    "            print(\"Bootstrap p-value:\", bootstrap_p_value)\n",
    "\n",
    "            # Determine whether the t-test results are statistically significant (typically p < 0.05)\n",
    "            if round(p_value, 2) <= 0.05:\n",
    "                print(f\"The means of {key1} and {key2} are statistically different.\")\n",
    "            else:\n",
    "                print(f\"There is no significant difference between the means of {key1} and {key2}.\")\n",
    "\n",
    "            # Convert the tuple keys to strings before updating the p_value_dict\n",
    "                 \n",
    "            if len(matching_criteria_1) >= 4 and matching_criteria_1[1] == 'Right':\n",
    "                str_key1 = ' '.join([matching_criteria_1[1] + ' ' + matching_criteria_1[2] + ' ' + matching_criteria_1[3],matching_criteria_1[4]])\n",
    "            elif len(matching_criteria_1)>= 3 and matching_criteria_1[1]== 'Face':\n",
    "                str_key1 = ' '.join([key_elements[0], matching_criteria_1[1] + ' ' + matching_criteria_1[2],matching_criteria_1[3]])\n",
    "            else:\n",
    "                str_key1 = ' '.join(matching_criteria_1[:3])\n",
    "\n",
    "\n",
    "            # Update the p_value_dict with the p-values using the selected keys\n",
    "            p_value_dict[str_key1] = {\n",
    "                'p_value': round(p_value, 2),\n",
    "                'bootstrap_p_value': round(bootstrap_p_value, 2)\n",
    "            }\n",
    "\n",
    "            # Save the updated p_value_dict back to the JSON file\n",
    "            with open(p_value_file, 'w') as file:\n",
    "                json.dump(p_value_dict, file)\n",
    "        else:\n",
    "            print(\"No auto-filled options available based on Key1.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input for Key1. Please enter a valid number.\")\n",
    "else:\n",
    "    print(\"No 'Post' keys without 'Sham' available in the dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D1', 'Locomotion', 'Right']\n",
      "['D1', 'Locomotion', 'Locomotion']\n",
      "['D1', 'Locomotion', 'Face']\n",
      "['D1', 'Face Groom', 'Right']\n",
      "['D1', 'Face Groom', 'Locomotion']\n",
      "['D1', 'Face Groom', 'Face']\n",
      "['Right', 'Turn', 'Bilateral', 'Right']\n",
      "['Right', 'Turn', 'Bilateral', 'Locomotion']\n",
      "['Right', 'Turn', 'Bilateral', 'Face']\n",
      "['Right', 'Turn', 'Contralateral', 'Right']\n",
      "['Right', 'Turn', 'Contralateral', 'Locomotion']\n",
      "['Right', 'Turn', 'Contralateral', 'Face']\n",
      "['Right', 'Turn', 'Ipsilateral', 'Right']\n",
      "['Right', 'Turn', 'Ipsilateral', 'Locomotion']\n",
      "['Right', 'Turn', 'Ipsilateral', 'Face']\n",
      "╒════════════╤═════════════╤═════════════════╤════════╤═══════════════╤══════════════╤═════════╕\n",
      "│            │  bilateral  │  contralateral  │  face  │  ipsilateral  │  locomotion  │  right  │\n",
      "╞════════════╪═════════════╪═════════════════╪════════╪═══════════════╪══════════════╪═════════╡\n",
      "│ face groom │             │                 │        │               │              │         │\n",
      "├────────────┼─────────────┼─────────────────┼────────┼───────────────┼──────────────┼─────────┤\n",
      "│ locomotion │             │                 │        │               │              │         │\n",
      "├────────────┼─────────────┼─────────────────┼────────┼───────────────┼──────────────┼─────────┤\n",
      "│    turn    │             │                 │        │               │              │         │\n",
      "╘════════════╧═════════════╧═════════════════╧════════╧═══════════════╧══════════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Read data from the JSON file\n",
    "with open('p_value_inner_trial.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Create dictionaries to store p-values and labels\n",
    "p_value_dict = {}\n",
    "row_labels = set()\n",
    "column_labels = set()\n",
    "\n",
    "# Define a function to normalize labels to lowercase\n",
    "def normalize_label(label):\n",
    "    return label.lower()\n",
    "\n",
    "# Extract and organize the p-values, row labels, and column labels\n",
    "for key, p_values in data.items():\n",
    "    elements = key.split(\" \")\n",
    "    if elements[1]=='Turn': \n",
    "        elements[1]= [elements[0] +\" \"+ elements[1]+\" \"+elements[2]]\n",
    "        elements=elements[1]\n",
    "    elif elements[1]=='Face':\n",
    "        elements=[elements[0],elements[1]+\" \"+elements[2],elements[3]]\n",
    "    \n",
    "    print(elements)\n",
    "    if len(elements) >= 3:\n",
    "        row_label = normalize_label(elements[1])  # Normalize to lowercase\n",
    "        col_label = normalize_label(elements[2])  # Normalize to lowercase\n",
    "        \n",
    "        row_labels.add(row_label)\n",
    "        column_labels.add(col_label)\n",
    "        \n",
    "        p_value_dict[(row_label, col_label)] = p_values.get(\"p_value_ttest\")\n",
    "\n",
    "# Sort the labels alphabetically\n",
    "sorted_row_labels = sorted(row_labels)\n",
    "sorted_column_labels = sorted(column_labels)\n",
    "\n",
    "# Create a list to hold the data in a tabular format\n",
    "table_data = []\n",
    "\n",
    "# Define a function to add color to the cell box based on significance\n",
    "def color_box(p_value):\n",
    "    if isinstance(p_value, float):\n",
    "        if p_value < 0.01:\n",
    "            return f\"\\033[41m{p_value:.2f}\\033[0m\"  # Red background for highly significant\n",
    "        elif p_value < 0.05:\n",
    "            return f\"\\033[43m{p_value:.2f}\\033[0m\"  # Yellow background for moderately significant\n",
    "        else:\n",
    "            return f\"{p_value:.2f}\"  # Default background for non-significant p-values\n",
    "    else:\n",
    "        return p_value\n",
    "\n",
    "# Build the table data with colored cell boxes\n",
    "for row_label in sorted_row_labels:\n",
    "    row = [row_label]\n",
    "    for col_label in sorted_column_labels:\n",
    "        p_value = p_value_dict.get((row_label, col_label), '')\n",
    "        colored_box = color_box(p_value)\n",
    "        row.append(colored_box)\n",
    "    table_data.append(row)\n",
    "\n",
    "# Print the Punnett square-like structure with improved readability and colored cell boxes\n",
    "table_headers = [\"\"] + sorted_column_labels\n",
    "table = tabulate(table_data, headers=table_headers, tablefmt=\"fancy_grid\", numalign=\"center\", stralign=\"center\")\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Matching Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Total Bout Count: 254.67\n",
      "Average Total Duration Count: 1876.33\n"
     ]
    }
   ],
   "source": [
    "averages_dict_file = \"averages_dict_file\"\n",
    "\n",
    "# Try to open the file for reading, if it exists\n",
    "try:\n",
    "    with open(averages_dict_file, 'r') as file:\n",
    "        averages_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    averages_dict = {}\n",
    "\n",
    "# Initialize dictionaries for stim_count and duration\n",
    "bout_count = {}\n",
    "duration = {}\n",
    "\n",
    "for row_index, tuples_list in matching_tuples.items():\n",
    "    # Initialize dictionaries for each row index\n",
    "    bout_count[row_index] = []\n",
    "    duration[row_index] = []\n",
    "\n",
    "    for matching_tuple in tuples_list:\n",
    "        if len(matching_tuple) > 3:\n",
    "            bout_count[row_index].append(matching_tuple[3])\n",
    "        if len(matching_tuple) > 4:\n",
    "            duration[row_index].append(matching_tuple[4])\n",
    "\n",
    "# Calculate totals for bout_count and duration for each row index\n",
    "total_bout_count = {}\n",
    "total_duration = {}\n",
    "\n",
    "for row_index in matching_tuples.keys():\n",
    "    total_bout_count[row_index] = len(bout_count[row_index])\n",
    "    total_duration[row_index] = sum(duration[row_index])\n",
    "\n",
    "# Extract total bout count values as a list\n",
    "total_bout_count_values = list(total_bout_count.values())\n",
    "average_total_bout_count = round((sum(total_bout_count_values) / len(total_bout_count_values)), 2)\n",
    "\n",
    "total_duration_values = list(total_duration.values())\n",
    "average_total_duration = round((sum(total_duration_values) / len(total_duration_values)), 2)\n",
    "\n",
    "# Update the averages_dict with the calculated averages\n",
    "# Convert tuple keys to strings and save with double quotes\n",
    "for i in range(len(filtering_criteria) - 1):  # Iterate up to the second-to-last element\n",
    "    if filtering_criteria[i] == \"Selected Header: Sham\" and filtering_criteria[i + 1] == \"Filter Value: y\":\n",
    "        averages_dict[\"D1, Face Groom Array, Bout, Sham\"] = total_bout_count_values\n",
    "        averages_dict[\"D1, Face Groom Array, Duration, Sham\"] = total_duration_values\n",
    "        break  # Exit the loop if the condition is met\n",
    "\n",
    "# If the loop completes without finding the condition, update the other keys\n",
    "else:\n",
    "    averages_dict[\"D1, Locomotoion, Face Groom Array, Bout\"] = total_bout_count_values\n",
    "    averages_dict[\"D1, Locomotion, Face Groom Array, Duration\"] = total_duration_values\n",
    "\n",
    "# Save the updated averages_dict back to the file with double quotes\n",
    "with open(averages_dict_file, 'w') as file:\n",
    "    json.dump(averages_dict, file)\n",
    "\n",
    "# Print the dictionary\n",
    "#print(\"Averages Dictionary:\")\n",
    "#print(averages_dict)\n",
    "\n",
    "# Print the average\n",
    "print(\"Average Total Bout Count:\", average_total_bout_count)\n",
    "print(\"Average Total Duration Count:\", average_total_duration)\n",
    "\n",
    "# Print totals for stim_count and duration for each row index\n",
    "#for row_index in matching_tuples.keys():\n",
    "    #print(f'Totals for Row Index {row_index}:')\n",
    "    #print('Total Bout Count:', total_bout_count[row_index])\n",
    "    #print('Total Duration:', total_duration[row_index])\n",
    "    #print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys without 'Sham' (showing first 3 elements):\n",
      "1. D1, Right Turn, Right Turn Array, Bout\n",
      "2. D1, Right Turn, Right Turn Array, Duration\n",
      "3. D1, Right Turn, Locomotion Array, Bout\n",
      "4. D1, Right Turn, Locomotion Array, Duration\n",
      "5. D1, Right Turn, Face Groom Array, Bout\n",
      "6. D1, Right Turn, Face Groom Array, Duration\n",
      "7. D1, Right Turn-Contralateral, Right Turn Array, Bout\n",
      "8. D1, Right Turn-Contralateral, Right Turn Array, Duration\n",
      "9. D1, Right Turn-Contralateral, Locomotion Array, Bout\n",
      "10. D1, Right Turn-Contralateral, Locomotion Array, Duration\n",
      "11. D1, Right Turn-Contralateral, Face Groom Array, Bout\n",
      "12. D1, Right Turn-Contralateral, Face Groom Array, Duration\n",
      "13. D1, Right Turn-Ipsilateral, Right Turn Array, Bout\n",
      "14. D1, Right Turn-Ipsilateral, Right Turn Array, Duration\n",
      "15. D1, Right Turn-Ipsilateral, Locomotion Array, Bout\n",
      "16. D1, Right Turn-Ipsilateral, Locomotion Array, Duration\n",
      "17. D1, Right Turn-Ipsilateral, Face Groom Array, Bout\n",
      "18. D1, Right Turn-Ipsilateral, Face Groom Array, Duration\n",
      "19. D1, Face Groom, Right Turn Array, Bout\n",
      "20. D1, Face Groom, Right Turn Array, Duration\n",
      "21. D1, Face Groom, Locomotion Array, Bout\n",
      "22. D1, Face Groom, Locomotion Array, Duration\n",
      "23. D1, Face Groom, Face Groom Array, Bout\n",
      "24. D1, Face Groom, Face Groom Array, Duration\n",
      "25. D1, Locomotoion, Right Turn Array, Bout\n",
      "26. D1, Locomotion, Right Turn Array, Duration\n",
      "27. D1, Locomotoion, Locomotion Array, Bout\n",
      "28. D1, Locomotion, Locomotion Array, Duration\n",
      "29. D1, Locomotoion, Face Groom Array, Bout\n",
      "30. D1, Locomotion, Face Groom Array, Duration\n",
      "Enter the number corresponding to Key1: 30\n",
      "You have selected Key1: D1, Locomotion, Face Groom Array, Duration\n",
      "['D1', 'Face Groom Array', 'Duration']\n",
      "Auto-filled options for Key2:\n",
      "1. D1, Face Groom Array, Duration\n",
      "Auto-filled Key2 based on matching criteria: D1, Face Groom Array, Duration, Sham\n",
      "T-Test Results:\n",
      "t-statistic: 0.5594203244802346\n",
      "p-value (T-Test): 0.5810585673050408\n",
      "Welch's T-Test Results:\n",
      "t-statistic (Welch's T-Test): 0.41633419590082166\n",
      "p-value (Welch's T-Test): 0.6916083990856303\n",
      "Bootstrapping Results:\n",
      "Bootstrap p-value: 0.7951988089400542\n",
      "There is no significant difference between the means of D1, Locomotion, Face Groom Array, Duration and D1, Face Groom Array, Duration, Sham (T-Test).\n",
      "There is no significant difference between the distributions of D1, Locomotion, Face Groom Array, Duration and D1, Face Groom Array, Duration, Sham (Welchs T Test).\n",
      "There is no significant difference between the means of D1, Locomotion, Face Groom Array, Duration and D1, Face Groom Array, Duration, Sham (Bootstrapping).\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the JSON file where you want to store the dictionary\n",
    "p_value_file = \"p_value_file.json\"\n",
    "\n",
    "# Initialize the p_value_dict as a global variable or load it from the JSON file if it exists\n",
    "try:\n",
    "    with open(p_value_file, 'r') as file:\n",
    "        p_value_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    p_value_dict = {}\n",
    "\n",
    "# Initialize the averages_dict as a global variable or load it from the JSON file if it exists\n",
    "try:\n",
    "    with open(averages_dict_file, 'r') as file:\n",
    "        averages_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # Handle the file not found error appropriately\n",
    "    averages_dict = {}\n",
    "\n",
    "# Extract keys without 'Sham' and keys with 'Sham' from averages_dict\n",
    "keys_without_sham = [key for key in averages_dict.keys() if 'Sham' not in key]\n",
    "keys_with_sham = [key for key in averages_dict.keys() if 'Sham' in key]\n",
    "\n",
    "# Check if there are keys without 'Sham' to use as Key1\n",
    "if keys_without_sham:\n",
    "    print(\"Available keys without 'Sham' (showing first 3 elements):\")\n",
    "    for idx, key in enumerate(keys_without_sham, start=1):\n",
    "        # Extract the first three elements of the key\n",
    "        key_elements = key.split(', ')\n",
    "        formatted_key = ', '.join(key_elements)\n",
    "        print(f\"{idx}. {formatted_key}\")\n",
    "\n",
    "    # Prompt the user to select Key1\n",
    "    try:\n",
    "        selected_key_idx1 = int(input(\"Enter the number corresponding to Key1: \")) - 1\n",
    "        key1 = keys_without_sham[selected_key_idx1]\n",
    "        print(f\"You have selected Key1: {key1}\")\n",
    "\n",
    "        # Auto-fill Key2 based on the matching criteria\n",
    "        matching_criteria_1 = key1.split(', ')\n",
    "        matching_criteria = [matching_criteria_1[0],matching_criteria_1[2], matching_criteria_1[3]]\n",
    "        print(matching_criteria)\n",
    "        auto_fill_keys = [key for key in keys_with_sham if\n",
    "                          matching_criteria == key.split(', ')[:3]]\n",
    "\n",
    "        if auto_fill_keys:\n",
    "            print(\"Auto-filled options for Key2:\")\n",
    "            for idx, key in enumerate(auto_fill_keys[:3], start=1):\n",
    "                # Extract the first three elements of the key\n",
    "                key_elements = key.split(', ')[:3]\n",
    "                formatted_key = ', '.join(key_elements)\n",
    "                print(f\"{idx}. {formatted_key}\")\n",
    "\n",
    "            # Automatically set Key2 based on the matching criteria\n",
    "            key2 = auto_fill_keys[0]  # You can adjust this if you have specific logic to choose among them\n",
    "            print(f\"Auto-filled Key2 based on matching criteria: {key2}\")\n",
    "\n",
    "            # Retrieve the samples associated with the selected keys\n",
    "            samples1 = averages_dict.get(key1, [])\n",
    "            samples2 = averages_dict.get(key2, [])\n",
    "            \n",
    "\n",
    "            # Perform bootstrapping with 1000 iterations\n",
    "            num_bootstrap_samples = 10000\n",
    "            bootstrap_mean_differences = []\n",
    "\n",
    "            # Perform bootstrapping with 'num_bootstrap_samples' iterations\n",
    "            for _ in range(num_bootstrap_samples):\n",
    "                # Generate bootstrap samples by resampling with replacement\n",
    "                bootstrap_samples1 = np.random.choice(samples1, len(samples1), replace=True)\n",
    "                bootstrap_samples2 = np.random.choice(samples2, len(samples2), replace=True)\n",
    "\n",
    "                # Calculate the mean difference between bootstrap_samples1 and bootstrap_samples2\n",
    "                bootstrap_mean_difference = np.mean(bootstrap_samples1) - np.mean(bootstrap_samples2)\n",
    "\n",
    "                # Append the mean difference to the list\n",
    "                bootstrap_mean_differences.append(bootstrap_mean_difference)\n",
    "\n",
    "            # Calculate the observed difference in means between the original samples\n",
    "            observed_diff = np.mean(samples1) - np.mean(samples2)\n",
    "\n",
    "            # Calculate the p-value for the bootstrap test\n",
    "            bootstrap_p_value_ttest = (np.sum(np.abs(bootstrap_mean_differences) >= np.abs(observed_diff)) + 1) / (\n",
    "                num_bootstrap_samples + 1)\n",
    "            \n",
    "            # Perform a t-test on the original samples\n",
    "            t_statistic_ttest, p_value_ttest = stats.ttest_ind(samples1, samples2)\n",
    "\n",
    "           \n",
    "            # Perform Welch's t-test\n",
    "            t_statistic_welch, p_value_welch = stats.ttest_ind(samples1, samples2, equal_var=False)\n",
    "            \n",
    "            # Print the t-test results\n",
    "            print(\"T-Test Results:\")\n",
    "            print(\"t-statistic:\", t_statistic_ttest)\n",
    "            print(\"p-value (T-Test):\", p_value_ttest)\n",
    "\n",
    "            # Print the Welch's T-Test Results\n",
    "            print(\"Welch's T-Test Results:\")\n",
    "            print(\"t-statistic (Welch's T-Test):\", t_statistic_welch)\n",
    "            print(\"p-value (Welch's T-Test):\", p_value_welch)\n",
    "\n",
    "\n",
    "            # Print the bootstrapping results\n",
    "            print(\"Bootstrapping Results:\")\n",
    "            print(\"Bootstrap p-value:\", bootstrap_p_value)\n",
    "\n",
    "            # Determine whether the t-test results are statistically significant (typically p < 0.05)\n",
    "            if round(p_value_ttest,2) <= 0.05:\n",
    "                print(f\"The means of {key1} and {key2} are statistically different (T-Test).\")\n",
    "            else:\n",
    "                print(f\"There is no significant difference between the means of {key1} and {key2} (T-Test).\")\n",
    "\n",
    "            # Determine whether the Mann-Whitney U test results are statistically significant (typically p < 0.05)\n",
    "            if round(bootstrap_p_value_ttest,2) <= 0.05:\n",
    "                print(f\"The distributions of {key1} and {key2} are statistically different (Welch's T Test.\")\n",
    "            else:\n",
    "                print(f\"There is no significant difference between the distributions of {key1} and {key2} (Welchs T Test).\")\n",
    "\n",
    "            # Determine whether the bootstrapping results are statistically significant (typically p < 0.05)\n",
    "            if round(bootstrap_p_value,2) <= 0.05:\n",
    "                print(f\"The means of {key1} and {key2} are statistically different (Bootstrapping).\")\n",
    "            else:\n",
    "                print(f\"There is no significant difference between the means of {key1} and {key2} (Bootstrapping).\")\n",
    "\n",
    "            # Convert the tuple keys to strings before updating the p_value_dict\n",
    "            str_key1 = ', '.join(matching_criteria_1)\n",
    "\n",
    "            # Update the p_value_dict with the p-values using the selected keys\n",
    "            p_value_dict[str_key1] = {\n",
    "                'p_value_ttest': round(p_value_ttest,2),\n",
    "                'p_value_welchsttest': round(p_value_welch,2),\n",
    "                'bootstrap_p_value': round(bootstrap_p_value,2)\n",
    "            }\n",
    "\n",
    "            # Save the updated p_value_dict back to the JSON file\n",
    "            with open(p_value_file, 'w') as file:\n",
    "                json.dump(p_value_dict, file)\n",
    "        else:\n",
    "            print(\"No auto-filled options available based on Key1.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input for Key1. Please enter a valid number.\")\n",
    "else:\n",
    "    print(\"No keys without 'Sham' available in the dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════════════════════╤════════════════════╤════════════════════╤════════════════════╕\n",
      "│                          │  face groom array  │  locomotion array  │  right turn array  │\n",
      "╞══════════════════════════╪════════════════════╪════════════════════╪════════════════════╡\n",
      "│        face groom        │        0.54        │        \u001b[43m0.03\u001b[0m        │        0.31        │\n",
      "├──────────────────────────┼────────────────────┼────────────────────┼────────────────────┤\n",
      "│        locomotion        │        0.58        │        0.72        │        0.93        │\n",
      "├──────────────────────────┼────────────────────┼────────────────────┼────────────────────┤\n",
      "│        right turn        │        0.93        │        0.89        │        0.92        │\n",
      "├──────────────────────────┼────────────────────┼────────────────────┼────────────────────┤\n",
      "│ right turn-contralateral │        \u001b[43m0.04\u001b[0m        │        0.07        │        0.42        │\n",
      "├──────────────────────────┼────────────────────┼────────────────────┼────────────────────┤\n",
      "│  right turn-ipsilateral  │        0.56        │        0.33        │        0.62        │\n",
      "╘══════════════════════════╧════════════════════╧════════════════════╧════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Read data from the JSON file\n",
    "with open('p_value_file.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Create dictionaries to store p-values and labels\n",
    "p_value_dict = {}\n",
    "row_labels = set()\n",
    "column_labels = set()\n",
    "\n",
    "# Define a function to normalize labels to lowercase\n",
    "def normalize_label(label):\n",
    "    return label.lower()\n",
    "\n",
    "# Extract and organize the p-values, row labels, and column labels\n",
    "for key, p_values in data.items():\n",
    "    elements = re.split(r',\\s*', key)\n",
    "    if len(elements) >= 3:\n",
    "        row_label = normalize_label(elements[1])  # Normalize to lowercase\n",
    "        col_label = normalize_label(elements[2])  # Normalize to lowercase\n",
    "        \n",
    "        row_labels.add(row_label)\n",
    "        column_labels.add(col_label)\n",
    "        \n",
    "        p_value_dict[(row_label, col_label)] = p_values.get(\"p_value_ttest\")\n",
    "\n",
    "# Sort the labels alphabetically\n",
    "sorted_row_labels = sorted(row_labels)\n",
    "sorted_column_labels = sorted(column_labels)\n",
    "\n",
    "# Create a list to hold the data in a tabular format\n",
    "table_data = []\n",
    "\n",
    "# Define a function to add color to the cell box based on significance\n",
    "def color_box(p_value):\n",
    "    if isinstance(p_value, float):\n",
    "        if p_value < 0.01:\n",
    "            return f\"\\033[41m{p_value:.2f}\\033[0m\"  # Red background for highly significant\n",
    "        elif p_value < 0.05:\n",
    "            return f\"\\033[43m{p_value:.2f}\\033[0m\"  # Yellow background for moderately significant\n",
    "        else:\n",
    "            return f\"{p_value:.2f}\"  # Default background for non-significant p-values\n",
    "    else:\n",
    "        return p_value\n",
    "\n",
    "# Build the table data with colored cell boxes\n",
    "for row_label in sorted_row_labels:\n",
    "    row = [row_label]\n",
    "    for col_label in sorted_column_labels:\n",
    "        p_value = p_value_dict.get((row_label, col_label), '')\n",
    "        colored_box = color_box(p_value)\n",
    "        row.append(colored_box)\n",
    "    table_data.append(row)\n",
    "\n",
    "# Print the Punnett square-like structure with improved readability and colored cell boxes\n",
    "table_headers = [\"\"] + sorted_column_labels\n",
    "table = tabulate(table_data, headers=table_headers, tablefmt=\"fancy_grid\", numalign=\"center\", stralign=\"center\")\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Bout Count:\n",
      "Row Index 3: 40\n",
      "Row Index 4: 80\n",
      "\n",
      "Total Duration:\n",
      "Row Index 3: 170\n",
      "Row Index 4: 358\n",
      "Average Total Bout Count: 60.0\n",
      "Average Total Duration Count: 264.0\n"
     ]
    }
   ],
   "source": [
    "averages_dict_file = \"averages_stim_dict_file\"\n",
    "\n",
    "# Try to open the file for reading, if it exists\n",
    "try:\n",
    "    with open(averages_dict_file, 'r') as file:\n",
    "        averages_dict = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    averages_dict = {}\n",
    "\n",
    "# Initialize dictionaries for stim_count and duration\n",
    "bout_count = {}\n",
    "duration = {}\n",
    "\n",
    "for row_index, tuples_list in matching_tuples.items():\n",
    "    # Initialize dictionaries for each row index\n",
    "    bout_count[row_index] = []\n",
    "    duration[row_index] = []\n",
    "\n",
    "    for matching_tuple in tuples_list:\n",
    "        if matching_tuple[5]>3000 and matching_tuple[6]<=21000:\n",
    "            if len(matching_tuple) > 3:\n",
    "                bout_count[row_index].append(matching_tuple[3])\n",
    "            if len(matching_tuple) > 4:\n",
    "                duration[row_index].append(matching_tuple[4])\n",
    "\n",
    "# Calculate totals for bout_count and duration for each row index\n",
    "total_bout_count = {}\n",
    "total_duration = {}\n",
    "\n",
    "for row_index in matching_tuples.keys():\n",
    "    total_bout_count[row_index] = len(bout_count[row_index])\n",
    "    total_duration[row_index] = sum(duration[row_index])\n",
    "    \n",
    "print(\"Total Bout Count:\")\n",
    "for row_index, bout_count_total in total_bout_count.items():\n",
    "    print(f\"Row Index {row_index}: {bout_count_total}\")\n",
    "\n",
    "print(\"\\nTotal Duration:\")\n",
    "for row_index, duration_total in total_duration.items():\n",
    "    print(f\"Row Index {row_index}: {duration_total}\")\n",
    "    \n",
    "    \n",
    "# Extract total bout count values as a list\n",
    "total_bout_count_values = list(total_bout_count.values())\n",
    "average_total_bout_count = round((sum(total_bout_count_values) / len(total_bout_count_values)), 2)\n",
    "\n",
    "total_duration_values = list(total_duration.values())\n",
    "average_total_duration = round((sum(total_duration_values) / len(total_duration_values)), 2)\n",
    "\n",
    "# Update the averages_dict with the calculated averages\n",
    "# Convert tuple keys to strings and save with double quotes\n",
    "for i in range(len(filtering_criteria) - 1):  # Iterate up to the second-to-last element\n",
    "    if filtering_criteria[i] == \"Selected Header: Sham\" and filtering_criteria[i + 1] == \"Filter Value: y\":\n",
    "        averages_dict[\"D1, Face Groom Array, Bout, Sham\"] = total_bout_count_values\n",
    "        averages_dict[\"D1, Face Groom Array, Duration, Sham\"] = total_duration_values\n",
    "        break  # Exit the loop if the condition is met\n",
    "\n",
    "# If the loop completes without finding the condition, update the other keys\n",
    "else:\n",
    "    averages_dict[\"D1, Locomotoion, Locomotionn Array, Bout\"] = total_bout_count_values\n",
    "    averages_dict[\"D1, Locomotion, Locomotion Array, Duration\"] = total_duration_values\n",
    "\n",
    "# Save the updated averages_dict back to the file with double quotes\n",
    "with open(averages_dict_file, 'w') as file:\n",
    "    json.dump(averages_dict, file)\n",
    "\n",
    "# Print the dictionary\n",
    "#print(\"Averages Dictionary:\")\n",
    "#print(averages_dict)\n",
    "\n",
    "# Print the average\n",
    "print(\"Average Total Bout Count:\", average_total_bout_count)\n",
    "print(\"Average Total Duration Count:\", average_total_duration)\n",
    "\n",
    "# Print totals for stim_count and duration for each row index\n",
    "#for row_index in matching_tuples.keys():\n",
    "    #print(f'Totals for Row Index {row_index}:')\n",
    "    #print('Total Bout Count:', total_bout_count[row_index])\n",
    "    #print('Total Duration:', total_duration[row_index])\n",
    "    #print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Index Name: Bouts, Population Size (N): 20, Population Average: 307.0, Population Std Deviation: 176.421855509051\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [291]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected Index Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_index_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Population Size (N): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Population Average: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpopulation_average\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Population Std Deviation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpopulation_std\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Integrate the new functionality to prompt the user for binning\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m user_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo you want to graph the selected index with per minute with a specific bin length for each key? (yes/no): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_choice\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Define the selected bin width in minutes\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     bin_width_minutes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the bin width in minutes: \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Check if a JSON file containing plotted_data exists\n",
    "try:\n",
    "    with open(\"plotted_data.json\", \"r\") as file:\n",
    "        plotted_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    # If the file doesn't exist, initialize an empty dictionary\n",
    "    plotted_data = {}\n",
    "# Assuming 'matching_tuples' contains the selected tuples\n",
    "# and 'time_values' contains the corresponding time values\n",
    "\n",
    "# Define the selected indices for start and end time\n",
    "start_time_index = 5\n",
    "end_time_index = 6\n",
    "bin_width_minutes =5\n",
    "# Define a dictionary to map index values to names\n",
    "index_name_mapping = {\n",
    "    0: 'Behavior',\n",
    "    1: 'Stim Count',\n",
    "    2: 'Frame Rate',\n",
    "    3: 'Bouts',\n",
    "    4: 'Duration',\n",
    "    5: 'Start Time',\n",
    "    6: 'End Time'\n",
    "}\n",
    "\n",
    "# Extract the name associated with the selected index\n",
    "selected_index_name = index_name_mapping[selected_index]\n",
    "\n",
    "selected_values = [tuple[selected_index]for tuple in values for row_index, values in matching_tuples.items()]\n",
    "\n",
    "# Extract the start and end times from matching_tuples and convert to minutes\n",
    "start_times = [tuple[start_time_index] / 600 for values in matching_tuples.values() for tuple in values]\n",
    "end_times = [tuple[end_time_index] / 600 for values in matching_tuples.values() for tuple in values]\n",
    "\n",
    "\n",
    "# Calculate the minimum and maximum times\n",
    "min_time = 0\n",
    "max_time = 65\n",
    "\n",
    "# Create time bins\n",
    "time_bins = np.arange(min_time, max_time + bin_width_minutes, bin_width_minutes)\n",
    "\n",
    "# Initialize lists to store mean values and SEM\n",
    "mean_values = []\n",
    "sem_values = []\n",
    "\n",
    "\n",
    "# Population size (N) is the total number of data points in your entire dataset\n",
    "N = len(matching_tuples)\n",
    "\n",
    "# Calculate the population average and standard deviation for the entire dataset\n",
    "population_average = np.mean(selected_values)\n",
    "population_std = np.std(selected_values, ddof=1)\n",
    "\n",
    "# Print the population information as a single line with the selected index name\n",
    "print(f\"Selected Index Name: {selected_index_name}, Population Size (N): {N}, Population Average: {population_average}, Population Std Deviation: {population_std}\")\n",
    "\n",
    "\n",
    "# Integrate the new functionality to prompt the user for binning\n",
    "user_choice = input(\"Do you want to graph the selected index with per minute with a specific bin length for each key? (yes/no): \")\n",
    "\n",
    "if user_choice.lower() == \"yes\":\n",
    "\n",
    "    # Define the selected bin width in minutes\n",
    "    bin_width_minutes = float(input(\"Enter the bin width in minutes: \"))\n",
    "    \n",
    "    # Initialize dictionaries to store mean values and SEM for each key\n",
    "    key_mean_values = {}\n",
    "    key_sem_values = {}\n",
    "    # Initialize a dictionary to store bin counts for each key\n",
    "    key_bin_counts = {}\n",
    "\n",
    "    # Loop through each key in the 'matching_tuples' dictionary\n",
    "    for key, values in matching_tuples.items():\n",
    "        # Extract the selected index name associated with the key\n",
    "        selected_index_name = index_name_mapping[selected_index]\n",
    "\n",
    "        # Extract the selected values at the selected index for the current key\n",
    "        selected_values = [tuple[selected_index] for tuple in values]\n",
    "\n",
    "        # Extract the start and end times for the current key and convert to minutes\n",
    "        start_times = [tuple[5] / 600 for tuple in values]\n",
    "        end_times = [tuple[6] / 600 for tuple in values]\n",
    "\n",
    "        # Create time bins\n",
    "        time_bins = np.arange(min_time, max_time + bin_width_minutes, bin_width_minutes)\n",
    "\n",
    "        # Initialize dictionaries to store bin values and bin start/end times\n",
    "        bin_data = {\n",
    "            'Bin Start Time': [],\n",
    "            'Bin End Time': [],\n",
    "            'Selected Values': [],\n",
    "            'Bin Values': []\n",
    "        }\n",
    "\n",
    "        # Initialize a list to store bin counts for the current key\n",
    "        bin_counts = []\n",
    "\n",
    "        # Iterate through the time bins for the current key\n",
    "        for bin_start, bin_end in zip(time_bins[:-1], time_bins[1:]):\n",
    "            bin_values = []\n",
    "\n",
    "            # Iterate through the data to collect values within this bin\n",
    "            for start_time, end_time, value_at_selected_index in zip(start_times, end_times, selected_values):\n",
    "                if start_time >= bin_start and end_time <= bin_end:\n",
    "                    # Value at the selected index falls within this bin for the current key\n",
    "                    bin_values.append(value_at_selected_index)\n",
    "\n",
    "            # Append data to the bin_data dictionary for printing\n",
    "            bin_data['Bin Start Time'].append(bin_start)\n",
    "            bin_data['Bin End Time'].append(bin_end)\n",
    "            bin_data['Selected Values'].append(selected_values)\n",
    "            bin_data['Bin Values'].append(bin_values)\n",
    "\n",
    "            # Calculate the number of elements in this bin and store it in bin_counts\n",
    "            bin_count = len(bin_values)\n",
    "            bin_counts.append(bin_count)\n",
    "\n",
    "        # Store the bin counts for the current key in the key_bin_counts dictionary\n",
    "        key_bin_counts[key] = bin_counts\n",
    "\n",
    "    # Initialize a dictionary to store the result\n",
    "    bin_counts_per_minute = {}\n",
    "\n",
    "    # Iterate through the keys and their corresponding bin counts\n",
    "    for key, bin_counts in key_bin_counts.items():\n",
    "        # Divide each bin count by bin_width_in_minutes\n",
    "        bin_counts_per_minute[key] = [count / bin_width_minutes for count in bin_counts]\n",
    "\n",
    "    # Initialize a list to store the counts per minute for each bin across all keys\n",
    "    all_bin_counts = [[] for _ in range(len(bin_counts_per_minute[next(iter(bin_counts_per_minute))]))]\n",
    "\n",
    "    # Populate the list with counts per minute from all keys for each bin\n",
    "    for counts_per_minute in bin_counts_per_minute.values():\n",
    "        for i, count in enumerate(counts_per_minute):\n",
    "            all_bin_counts[i].append(count)\n",
    "\n",
    "    # Initialize lists to store the bin mean and SEM\n",
    "    bin_means = []\n",
    "    bin_sem = []\n",
    "\n",
    "    # Calculate the mean and SEM for each bin across all keys\n",
    "    for bin_counts in all_bin_counts:\n",
    "        bin_mean = np.mean(bin_counts)  # Calculate the mean for the bin\n",
    "        bin_means.append(bin_mean)  # Store the bin mean in the bin_means list\n",
    "\n",
    "        # Use scipy to calculate the SEM for the bin\n",
    "        sem = stats.sem(bin_counts)  # SEM\n",
    "        bin_sem.append(sem)  # Store the SEM in the bin_sem list\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i, (bin_mean, sem) in enumerate(zip(bin_means, bin_sem), start=1):\n",
    "        data.append([f\"Bin {i}\", f\"{bin_mean:.2f} cpm\", f\"{sem:.2f}\"])\n",
    "\n",
    "    table = tabulate(data, headers=[\"Bin\", \"Avg (cpm)\", \"SEM\"], tablefmt=\"grid\")\n",
    "\n",
    "    print(table)\n",
    "\n",
    "    # Create a plot of counts per minute against time with error bars representing SEM\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(\n",
    "        time_bins[:-1] + bin_width_minutes / 2,  # X-axis values (middle of time bins)\n",
    "        bin_means,                               # Y-axis values (mean values for each bin)\n",
    "        yerr=bin_sem,                            # Error bars (SEM for each bin)\n",
    "        marker='o',\n",
    "        markersize=4,\n",
    "        capsize=4,\n",
    "        label=f'{selected_index_name} per Minute'\n",
    "    )\n",
    "    plt.xlabel('Time (minutes)')\n",
    "    plt.ylabel(f'{selected_index_name}')\n",
    "\n",
    "    # Check if the plot represents a Sham session from filtering criteria\n",
    "    sham_detected = False  # Initialize a flag\n",
    "    i = 0  # Initialize an index variable\n",
    "\n",
    "    while i < len(filtering_criteria):\n",
    "        criterion = filtering_criteria[i]\n",
    "\n",
    "        if criterion == \"Selected Header: Sham\" and i + 1 < len(filtering_criteria) and filtering_criteria[i + 1] == \"Filter Value: y\":\n",
    "            sham_detected = True\n",
    "            break  # Exit the loop if Sham is detected\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Set the plot title based on Sham detection\n",
    "    if sham_detected:\n",
    "        plot_title = f'{selected_index_name} per minute over time ({bin_width_minutes}-minute Intervals) with SEM - Sham'\n",
    "    else:\n",
    "        plot_title = f'{selected_index_name} per minute over time ({bin_width_minutes}-minute Intervals) with SEM'\n",
    "\n",
    "    plt.title(plot_title)  # Set the plot title\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Add a text box with filtering criteria to the far right\n",
    "    text_box_content = \"\\n\".join(filtering_criteria)\n",
    "    plt.gca().text(1.02, 0.5, text_box_content, transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.8),\n",
    "                    verticalalignment='center', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()  # Ensures that the text box does not interfere with other elements\n",
    "    plt.show()\n",
    "\n",
    "    # Check if the title already exists in the dictionary\n",
    "    if plot_title in plotted_data:\n",
    "        # If the title exists, find a unique title by appending a number\n",
    "        i = 2\n",
    "        while f\"{plot_title} ({i})\" in plotted_data:\n",
    "            i += 1\n",
    "        unique_plot_title = f\"{plot_title} ({i})\"\n",
    "    else:\n",
    "        unique_plot_title = plot_title\n",
    "        \n",
    "    # Convert NumPy arrays to lists\n",
    "    x_axis_values = (time_bins[:-1] + bin_width_minutes / 2).tolist()\n",
    "\n",
    "    # Update the plotted_data dictionary with lists instead of NumPy arrays\n",
    "    plotted_data[unique_plot_title] = {\n",
    "        'X-axis_values': x_axis_values,\n",
    "        'Y-axis_values': bin_means,\n",
    "        'SEM_values': bin_sem,\n",
    "        'Label': f'{selected_index_name} per Minute',\n",
    "        'X-axis_label': 'Time (minutes)',\n",
    "        'Y-axis_label': selected_index_name,\n",
    "        'Filtering_criteria': text_box_content,\n",
    "        'Bin_data': bin_data  # You can store the bin data if needed\n",
    "    }\n",
    "\n",
    "    # Save the updated plotted_data to the JSON file\n",
    "    with open(\"plotted_data.json\", \"w\") as file:\n",
    "        json.dump(plotted_data, file, indent=4)\n",
    "    \n",
    "else:\n",
    "    binned_data = {}\n",
    "    bin_width_minutes = float(input(\"Enter the bin width in minutes: \"))\n",
    "    for key, tuples in matching_tuples.items():\n",
    "        # Extract the values for the selected index from each tuple in the list\n",
    "        selected_values = [tuple[selected_index] for tuple in tuples]\n",
    "        # Initialize a dictionary to store binned values for this key\n",
    "        binned_values_dict = {}\n",
    "        # Create time bins\n",
    "        time_bins = np.arange(min_time, max_time + bin_width_minutes, bin_width_minutes)\n",
    "\n",
    "        # Iterate through the time bins\n",
    "        for bin_start, bin_end in zip(time_bins[:-1], time_bins[1:]):\n",
    "            # Initialize a list to store values in this bin\n",
    "            bin_values = []\n",
    "\n",
    "            # Iterate through the data points for this key\n",
    "            for start_time, end_time, value_at_selected_index in zip(start_times, end_times, selected_values):\n",
    "                if start_time > bin_start and end_time <= bin_end:\n",
    "                    # Value at the selected index falls within this bin for the current key\n",
    "                    bin_values.append(value_at_selected_index)\n",
    "\n",
    "            # Store the binned values for this bin in the dictionary\n",
    "            binned_values_dict[(bin_start, bin_end)] = bin_values\n",
    "\n",
    "        # Store the binned data for this key in the main dictionary\n",
    "        binned_data[key] = binned_values_dict\n",
    "\n",
    "    # Check the number of keys in the binned_data dictionary\n",
    "    num_keys = len(binned_data)\n",
    "\n",
    "    if num_keys == 1:\n",
    "        # If there is only one key, calculate SEM based on the binned values for each bin\n",
    "        single_key = next(iter(binned_data))  # Get the single key\n",
    "        single_key_binned_values = binned_data[single_key]  # Binned values for the single key\n",
    "\n",
    "        # Initialize a dictionary to store SEM values for each bin for the single key\n",
    "        sem_per_bin = {}\n",
    "\n",
    "        # Initialize a list to store data for the DataFrame\n",
    "        data = []\n",
    "\n",
    "        # Initialize lists to store bin information and values for plotting\n",
    "        bin_starts = []\n",
    "        bin_ends = []\n",
    "        bin_midpoints = []\n",
    "        bin_averages = []\n",
    "        bin_sems = []\n",
    "\n",
    "        # Iterate through the bins and their values\n",
    "        for bin_key, bin_values in single_key_binned_values.items():\n",
    "            # Calculate the SEM for this bin's values\n",
    "            if len(bin_values) > 1:\n",
    "                sem = np.std(bin_values, ddof=1) / np.sqrt(len(bin_values))\n",
    "            else:\n",
    "                sem = np.nan  # Set to NaN if there's insufficient data in the bin\n",
    "\n",
    "            # Store the SEM value in the dictionary with the bin key as the identifier\n",
    "            sem_per_bin[bin_key] = sem\n",
    "\n",
    "            # Extract bin start and end for table\n",
    "            bin_start, bin_end = bin_key\n",
    "            bin_starts.append(bin_start)\n",
    "            bin_ends.append(bin_end)\n",
    "\n",
    "            # Calculate bin midpoint for plotting\n",
    "            bin_midpoint = (bin_start + bin_end) / 2\n",
    "            bin_midpoints.append(bin_midpoint)\n",
    "\n",
    "            # Calculate the average for this bin\n",
    "            bin_average = np.mean(bin_values)\n",
    "            bin_averages.append(bin_average)\n",
    "\n",
    "            # Store the SEM value for plotting\n",
    "            bin_sems.append(sem)\n",
    "\n",
    "            # Append the data to the list for the DataFrame\n",
    "            data.append([bin_start, bin_end, bin_average, sem])  # Include SEM in the data\n",
    "\n",
    "        # Create a DataFrame from the list\n",
    "        df = pd.DataFrame(data, columns=['Bin Start', 'Bin End', 'Average', 'SEM'])\n",
    "\n",
    "        # Print the table\n",
    "        print(df)\n",
    "\n",
    "        # Define the color for the line and error bars\n",
    "        line_color = 'blue'\n",
    "\n",
    "        # Create a line plot with error bars\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(bin_midpoints, bin_averages, marker='o', color=line_color, label='Average')\n",
    "        plt.errorbar(bin_midpoints, bin_averages, yerr=bin_sems, fmt='o', color=line_color, capsize=4, label='SEM')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Time (minutes)')\n",
    "        plt.ylabel(f'Mean of {selected_index_name}')\n",
    "\n",
    "        sham_detected = False  # Initialize a flag\n",
    "        i = 0  # Initialize an index variable\n",
    "\n",
    "        while i < len(filtering_criteria):\n",
    "            criterion = filtering_criteria[i]\n",
    "\n",
    "            if criterion == \"Selected Header: Sham\" and i + 1 < len(filtering_criteria) and filtering_criteria[i + 1] == \"Filter Value: y\":\n",
    "                sham_detected = True\n",
    "                break  # Exit the loop if Sham is detected\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        # Set the plot title based on Sham detection\n",
    "        if sham_detected:\n",
    "            plot_title = f'{selected_index_name} Over Time ({bin_width_minutes}-minute Intervals) with SEM - Sham'\n",
    "        else:\n",
    "            plot_title = f'{selected_index_name} Over Time ({bin_width_minutes}-minute Intervals) with SEM'\n",
    "        plt.title(plot_title)\n",
    "        # Add legend\n",
    "        plt.legend()\n",
    "\n",
    "        # Add a text box with filtering criteria to the far right\n",
    "        text_box_content = \"\\n\".join(filtering_criteria)\n",
    "        plt.gca().text(1.02, 0.5, text_box_content, transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.8),\n",
    "                        verticalalignment='center', fontsize=10)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()  # Ensures all elements fit within the figure\n",
    "        plt.show()\n",
    "\n",
    "        # Check if the title already exists in the dictionary\n",
    "        if plot_title in plotted_data:\n",
    "            # If the title exists, find a unique title by appending a number\n",
    "            i = 2\n",
    "            while f\"{plot_title} ({i})\" in plotted_data:\n",
    "                i += 1\n",
    "            unique_plot_title = f\"{plot_title} ({i})\"\n",
    "        else:\n",
    "            unique_plot_title = plot_title\n",
    "\n",
    "        plotted_data[unique_plot_title] = {\n",
    "            'X-axis_values': bin_midpoints,  # Use bin_midpoints as X-axis values\n",
    "            'Y-axis_values': bin_averages,   # Use bin_averages as Y-axis values\n",
    "            'SEM_values': bin_sems,          # Use bin_sems as SEM values\n",
    "            'Label': f'{selected_index_name} over time',\n",
    "            'X-axis_label': 'Time (minutes)',\n",
    "            'Y-axis_label': selected_index_name,\n",
    "            'Filtering_criteria': text_box_content,\n",
    "            'Bin_data': bin_data,  # You can store the bin data if needed\n",
    "            'Sham_detected': sham_detected  # Include Sham detection flag in the dictionary\n",
    "        }\n",
    "\n",
    "        # Save the updated plotted_data to the JSON file\n",
    "        with open(\"plotted_data.json\", \"w\") as file:\n",
    "            json.dump(plotted_data, file, indent=4)\n",
    "    else:\n",
    "        # If there are multiple keys, perform the original SEM calculation across keys\n",
    "        average_per_key = {}\n",
    "\n",
    "        # Iterate through the binned data dictionary\n",
    "        for key, binned_values_dict in binned_data.items():\n",
    "            #print(f\"Key: {key}\")\n",
    "\n",
    "            # Iterate through the bins and their values\n",
    "            for bin_key, bin_values in binned_values_dict.items():\n",
    "                bin_start, bin_end = bin_key\n",
    "                #print(f\"Bin ({bin_start:.2f} - {bin_end:.2f} minutes): {bin_values}\")\n",
    "\n",
    "            #print()  # Print an empty line to separate each key's output   \n",
    "\n",
    "        # Initialize a dictionary to store the average of each bin for each key\n",
    "        average_per_key = {}\n",
    "\n",
    "        # Iterate through the binned data dictionary\n",
    "        for key, binned_values_dict in binned_data.items():\n",
    "            key_averages = {}  # Initialize a dictionary to store averages for this key\n",
    "\n",
    "            # Iterate through the bins and their values for this key\n",
    "            for bin_key, bin_values in binned_values_dict.items():\n",
    "                # Check if the bin_values list is empty before calculating the mean\n",
    "                if len(bin_values) > 0:\n",
    "                    # Calculate the average for the current bin\n",
    "                    bin_average = np.mean(bin_values)\n",
    "                else:\n",
    "                    bin_average = np.nan  # Set to NaN if the bin is empty\n",
    "\n",
    "                # Store the bin average in the dictionary with the bin key as the identifier\n",
    "                key_averages[bin_key] = bin_average\n",
    "\n",
    "            # Store the average values for this key in the main dictionary\n",
    "            average_per_key[key] = key_averages\n",
    "\n",
    "            # Iterate through the average_per_key dictionary\n",
    "        for key, key_averages in average_per_key.items():\n",
    "            print(f\"Key: {key}\")\n",
    "\n",
    "            # Iterate through the bins and their averages for this key\n",
    "            for bin_key, bin_average in key_averages.items():\n",
    "                bin_start, bin_end = bin_key\n",
    "                print(f\"Bin ({bin_start:.2f} - {bin_end:.2f} minutes) Average: {bin_average:.2f}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "        average_of_averages = {}  # Initialize a dictionary to store the average of averages for each bin\n",
    "        sem_of_averages = {}      # Initialize a dictionary to store the SEM for each bin's average\n",
    "\n",
    "        # Iterate through the bins (based on the first key, assuming all keys have the same bins)\n",
    "        for bin_key in average_per_key[next(iter(average_per_key))]:\n",
    "            bin_averages = []  # Initialize a list to store averages for this bin across keys\n",
    "\n",
    "            # Iterate through the keys and their corresponding averages\n",
    "            for key, key_averages in average_per_key.items():\n",
    "                bin_average = key_averages.get(bin_key, np.nan)  # Get the average for this key and bin\n",
    "                if not np.isnan(bin_average):\n",
    "                    bin_averages.append(bin_average)\n",
    "\n",
    "            # Calculate the average for this bin across keys and store it in the dictionary\n",
    "            if bin_averages:\n",
    "                bin_average_across_keys = np.mean(bin_averages)\n",
    "                average_of_averages[bin_key] = bin_average_across_keys\n",
    "\n",
    "                # Calculate the SEM for this bin's average\n",
    "                sem = stats.sem(bin_averages)\n",
    "                sem_of_averages[bin_key] = sem\n",
    "\n",
    "        # Print the averages and SEMs for each bin\n",
    "        for bin_key, bin_average in average_of_averages.items():\n",
    "            bin_start, bin_end = bin_key\n",
    "            bin_sem = sem_of_averages.get(bin_key, np.nan)\n",
    "            #print(f\"Bin ({bin_start:.2f} - {bin_end:.2f} minutes) Average of Averages: {bin_average:.2f} SEM: {bin_sem:.2f}\")\n",
    "\n",
    "      # Create a list to store data for the DataFrame\n",
    "        data = []\n",
    "\n",
    "        # Iterate through the bins and their averages\n",
    "        for bin_key, bin_average in average_of_averages.items():\n",
    "            bin_start, bin_end = bin_key\n",
    "            bin_sem = sem_of_averages.get(bin_key, np.nan)\n",
    "\n",
    "            # Append the data to the list\n",
    "            data.append([bin_start, bin_end, bin_average, bin_sem])\n",
    "\n",
    "        # Create a DataFrame from the list\n",
    "        df = pd.DataFrame(data, columns=['Bin Start', 'Bin End', 'Average', 'SEM'])\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(df)\n",
    "\n",
    "        # Extract bin information and values\n",
    "        bin_starts, bin_ends = zip(*average_of_averages.keys())\n",
    "        bin_averages = list(average_of_averages.values())\n",
    "        bin_sems = list(sem_of_averages.values())\n",
    "\n",
    "        # Convert bin starts and ends to midpoint values for plotting\n",
    "        bin_midpoints = [(start + end) / 2 for start, end in zip(bin_starts, bin_ends)]\n",
    "\n",
    "        # Define the color for the line and error bars\n",
    "        line_color = 'blue'\n",
    "\n",
    "        # Create a line plot with error bars\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(bin_midpoints, bin_averages, marker='o', color=line_color, label='Average')\n",
    "        plt.errorbar(bin_midpoints, bin_averages, yerr=bin_sems, fmt='o', color=line_color, capsize=4, label='SEM')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('Time (minutes)')\n",
    "        plt.ylabel(f'Mean of {selected_index_name}')\n",
    "        sham_detected = False  # Initialize a flag\n",
    "        i = 0  # Initialize an index variable\n",
    "\n",
    "        while i < len(filtering_criteria):\n",
    "            criterion = filtering_criteria[i]\n",
    "\n",
    "            if criterion == \"Selected Header: Sham\" and i + 1 < len(filtering_criteria) and filtering_criteria[i + 1] == \"Filter Value: y\":\n",
    "                sham_detected = True\n",
    "                break  # Exit the loop if Sham is detected\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        # Set the plot title based on Sham detection\n",
    "        if sham_detected:\n",
    "            plot_title = f'{selected_index_name} Over Time ({bin_width_minutes}-minute Intervals) with SEM - Sham'\n",
    "        else:\n",
    "            plot_title = f'{selected_index_name} Over Time ({bin_width_minutes}-minute Intervals) with SEM'\n",
    "        plt.title(plot_title)\n",
    "        # Add legend\n",
    "        plt.legend()\n",
    "\n",
    "        # Add a text box with filtering criteria to the far right\n",
    "        text_box_content = \"\\n\".join(filtering_criteria)\n",
    "        plt.gca().text(1.02, 0.5, text_box_content, transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.8),\n",
    "                        verticalalignment='center', fontsize=10)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()  # Ensures all elements fit within the figure\n",
    "        plt.show()\n",
    "\n",
    "        # Check if the title already exists in the dictionary\n",
    "        if plot_title in plotted_data:\n",
    "            # If the title exists, find a unique title by appending a number\n",
    "            i = 2\n",
    "            while f\"{plot_title} ({i})\" in plotted_data:\n",
    "                i += 1\n",
    "            unique_plot_title = f\"{plot_title} ({i})\"\n",
    "        else:\n",
    "            unique_plot_title = plot_title\n",
    "\n",
    "        plotted_data[unique_plot_title] = {\n",
    "        'X-axis_values': bin_midpoints,  # Use bin_midpoints as X-axis values\n",
    "        'Y-axis_values': bin_averages,   # Use bin_averages as Y-axis values\n",
    "        'SEM_values': bin_sems,          # Use bin_sems as SEM values\n",
    "        'Label': f'{selected_index_name} over tie',\n",
    "        'X-axis_label': 'Time (minutes)',\n",
    "        'Y-axis_label': selected_index_name,\n",
    "        'Filtering_criteria': text_box_content,\n",
    "        'Bin_data': bin_data,  # You can store the bin data if needed\n",
    "        'Sham_detected': sham_detected  # Include Sham detection flag in the dictionary\n",
    "    }\n",
    "\n",
    "        # Save the updated plotted_data to the JSON file\n",
    "        with open(\"plotted_data.json\", \"w\") as file:\n",
    "            json.dump(plotted_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dictionaries for plotting:\n",
      "1. Bouts Over Time (5.0-minute Intervals) with SEM\n",
      "2. Duration Over Time (3.0-minute Intervals) with SEM\n",
      "3. Duration Over Time (3.0-minute Intervals) with SEM (2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [306]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Ask the user how they want to select dictionaries for plotting\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 21\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to select all dictionaries, a range (e.g., \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2-5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m), or individual numbers (e.g., \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1 3 4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     24\u001b[0m         selected_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(available_dicts)))  \u001b[38;5;66;03m# Select all dictionaries\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Load the plotted_data dictionary from the JSON file\n",
    "try:\n",
    "    with open(\"plotted_data.json\", \"r\") as file:\n",
    "        plotted_data = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    plotted_data = {}  # Initialize an empty dictionary if the file doesn't exist\n",
    "\n",
    "# List available dictionaries for plotting\n",
    "available_dicts = list(plotted_data.keys())\n",
    "\n",
    "if not available_dicts:\n",
    "    print(\"No data available for plotting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Available dictionaries for plotting:\")\n",
    "for i, dictionary in enumerate(available_dicts, 1):\n",
    "    print(f\"{i}. {dictionary}\")\n",
    "\n",
    "# Ask the user how they want to select dictionaries for plotting\n",
    "while True:\n",
    "    user_input = input(\"Enter 'all' to select all dictionaries, a range (e.g., '2-5'), or individual numbers (e.g., '1 3 4'): \").strip()\n",
    "\n",
    "    if user_input.lower() == 'all':\n",
    "        selected_indices = list(range(len(available_dicts)))  # Select all dictionaries\n",
    "        break\n",
    "    elif '-' in user_input:\n",
    "        range_parts = user_input.split('-')\n",
    "        if len(range_parts) == 2 and range_parts[0].isnumeric() and range_parts[1].isnumeric():\n",
    "            start_idx = int(range_parts[0]) - 1\n",
    "            end_idx = int(range_parts[1])\n",
    "            if 0 <= start_idx < end_idx <= len(available_dicts):\n",
    "                selected_indices = list(range(start_idx, end_idx))  # Select a range\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid range. Please enter a valid range.\")\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter a valid range (e.g., '2-5').\")\n",
    "    else:\n",
    "        selected_numbers = user_input.split()\n",
    "        if all(num.isnumeric() for num in selected_numbers):\n",
    "            selected_indices = [int(num) - 1 for num in selected_numbers if 0 < int(num) <= len(available_dicts)]\n",
    "            if selected_indices:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid numbers. Please enter valid numbers.\")\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 'all', a valid range, or valid numbers.\")\n",
    "\n",
    "# Initialize a dictionary to store SEM values for each time\n",
    "sem_values_by_time = {}\n",
    "\n",
    "# Initialize lists to store combined data\n",
    "combined_data = []\n",
    "\n",
    "# Initialize the plot title to None\n",
    "plot_title = None\n",
    "\n",
    "# Iterate through the selected dictionaries and combine the data\n",
    "for idx in selected_indices:\n",
    "    dictionary = available_dicts[idx]\n",
    "    data = plotted_data[dictionary]\n",
    "\n",
    "    x_axis_values = data.get('X-axis_values', [])\n",
    "    y_axis_values = data.get('Y-axis_values', [])\n",
    "    sem_values = data.get('SEM_values', [])\n",
    "    label = data.get('Label', '')\n",
    "\n",
    "    # Append the data as a tuple (x, y, sem, label) to the combined_data list\n",
    "    combined_data.append((x_axis_values, y_axis_values, sem_values, label))\n",
    "\n",
    "    # Group Y values by time\n",
    "    for i, time in enumerate(x_axis_values):\n",
    "        if time not in sem_values_by_time:\n",
    "            sem_values_by_time[time] = []\n",
    "        sem_values_by_time[time].append(y_axis_values[i])\n",
    "\n",
    "# Create a single plot for all the combined data\n",
    "for x_values, y_values, sem, label in combined_data:\n",
    "    plt.errorbar(\n",
    "        x_values,\n",
    "        y_values,\n",
    "        yerr=sem if len(sem) > 0 else None,  # Use SEM values from the dictionary if available\n",
    "        marker='o',\n",
    "        markersize=4,\n",
    "        capsize=4,\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "# Set labels and title for the plot\n",
    "plt.xlabel(data.get('X-axis_label', ''))\n",
    "plt.ylabel(data.get('Y-axis_label', ''))\n",
    "\n",
    "# Extract the part of the title before \"SEM\"\n",
    "if plot_title:\n",
    "    plot_title = plot_key.split(\"with\")[0].strip()\n",
    "plt.title(plot_title)  # Set the plot title\n",
    "\n",
    "# Show grid and legend\n",
    "plt.grid(True)\n",
    "# Place the legend outside the plot and specify the bbox_to_anchor\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Calculate the SEM for each time and store the values\n",
    "sem_values_by_time_result = []\n",
    "for time, values in sem_values_by_time.items():\n",
    "    if len(values) < 2:\n",
    "        continue\n",
    "\n",
    "    sem = np.std(values) / np.sqrt(len(values))\n",
    "    sem_values_by_time_result.append([time, sem])  # Store time and SEM in a list\n",
    "\n",
    "# Print the SEM values as a table with a title\n",
    "table = tabulate(sem_values_by_time_result, headers=[\"Time\", \"SEM\"], tablefmt=\"fancy_grid\")\n",
    "print(\"SEM Values of Combined Plot\")\n",
    "print(table)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the plotted_data dictionary\n",
    "pprint.pprint(len(plotted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To print specific keys and their corresponding values, you can iterate through the dictionary\n",
    "for key, value in plotted_data.items():\n",
    "    print(f\"Key: {key}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(plotted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotted_data.clear()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check if the plotted_data dictionary includes keys with \"Bout per Minute\"\n",
    "keys_to_remove = [key for key in plotted_data if \"per minute over time\" in key]\n",
    "\n",
    "# Remove keys containing \"Bout per Minute\" and their values from plotted_data\n",
    "for key_to_remove in keys_to_remove:\n",
    "    del plotted_data[key_to_remove]\n",
    "\n",
    "print(\"Plotted data cleared for keys containing 'per minute over time'.\")\n",
    "\n",
    "# Save the updated plotted_data to the JSON file\n",
    "with open(\"plotted_data.json\", \"w\") as file:\n",
    "    json.dump(plotted_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check if the plotted_data dictionary includes keys with \"Bout per Minute\"\n",
    "keys_to_remove = [key for key in plotted_data if \"Over Time\" in key]\n",
    "\n",
    "# Remove keys containing \"Bout per Minute\" and their values from plotted_data\n",
    "for key_to_remove in keys_to_remove:\n",
    "    del plotted_data[key_to_remove]\n",
    "\n",
    "print(\"Plotted data cleared for keys containing 'Over Time'.\")\n",
    "\n",
    "# Save the updated plotted_data to the JSON file\n",
    "with open(\"plotted_data.json\", \"w\") as file:\n",
    "    json.dump(plotted_data, file, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define a dictionary to map index values to names\n",
    "index_name_mapping = {\n",
    "    0: 'Behavior',\n",
    "    1: 'Stim Count',\n",
    "    2: 'Frame Rate',\n",
    "    3: 'Bouts',\n",
    "    4: 'Duration',\n",
    "    5: 'Start Time',\n",
    "    6: 'End Time'\n",
    "}\n",
    "\n",
    "def process_matching_tuples(matching_tuples):\n",
    "    # Prompt the user to upload the first matching tuple and select the variable\n",
    "    first_variable = input(\"Upload the first matching tuple. Enter a variable name for it: \")\n",
    "    selected_index = int(input(f\"Select an index for {first_variable}: \"))\n",
    "    selected_index_name = index_name_mapping[selected_index]\n",
    "    selected_values = [tuple[selected_index] for tuple in matching_tuples]\n",
    "\n",
    "    # Prompt the user to upload the second matching tuple and select the variable\n",
    "    second_variable = input(\"Upload the second matching tuple. Enter a variable name for it: \")\n",
    "    selected_index2 = int(input(f\"Select an index for {second_variable}: \"))\n",
    "    selected_index_name2 = index_name_mapping[selected_index2]\n",
    "    selected_values2 = [tuple[selected_index2] for tuple in matching_tuples]\n",
    "\n",
    "    # Extract the start and end times from matching_tuples and convert to minutes\n",
    "    start_times = [tuple[start_time_index] / 600 for tuple in matching_tuples]\n",
    "    end_times = [tuple[end_time_index] / 600 for tuple in matching_tuples]\n",
    "\n",
    "    # Define the bin width in minutes\n",
    "    bin_width_minutes = 3  # Adjust the bin width as needed\n",
    "\n",
    "    # Calculate the minimum and maximum times\n",
    "    min_time = 0\n",
    "    max_time = 65\n",
    "\n",
    "    # Create time bins\n",
    "    time_bins = np.arange(min_time, max_time + bin_width_minutes, bin_width_minutes)\n",
    "\n",
    "    # Initialize lists to store counts, start times, end times, and SEM per bin for both variables\n",
    "    counts1 = []\n",
    "    counts2 = []\n",
    "    start_times_bin = []\n",
    "    end_times_bin = []\n",
    "    sem_values1 = []  # To store SEM per bin for variable 1\n",
    "    sem_values2 = []  # To store SEM per bin for variable 2\n",
    "\n",
    "    # Iterate through the time bins\n",
    "    for bin_start, bin_end in zip(time_bins[:-1], time_bins[1:]):\n",
    "        bin_values1 = []\n",
    "        bin_values2 = []\n",
    "\n",
    "        # Iterate through the data to collect values within this bin for both variables\n",
    "        for start_time, end_time, value1, value2 in zip(start_times, end_times, selected_values, selected_values2):\n",
    "            if start_time > bin_start and end_time <= bin_end:\n",
    "                # Data falls within this bin\n",
    "                bin_values1.append(value1)\n",
    "                bin_values2.append(value2)\n",
    "\n",
    "        # Calculate the count of elements within the bin for both variables\n",
    "        bin_count1 = len(bin_values1)\n",
    "        bin_count2 = len(bin_values2)\n",
    "\n",
    "        # Calculate the standard error of the mean (SEM) for both variables\n",
    "        sem1 = 0  # Initialize SEM for variable 1 for this bin\n",
    "        sem2 = 0  # Initialize SEM for variable 2 for this bin\n",
    "        if bin_count1 > 0:\n",
    "            sem1 = np.std(bin_values1, ddof=1) / np.sqrt(bin_count1) / bin_width_minutes\n",
    "        if bin_count2 > 0:\n",
    "            sem2 = np.std(bin_values2, ddof=1) / np.sqrt(bin_count2) / bin_width_minutes\n",
    "\n",
    "        # Append values to respective lists\n",
    "        counts1.append(bin_count1)\n",
    "        counts2.append(bin_count2)\n",
    "        start_times_bin.append(bin_start)\n",
    "        end_times_bin.append(bin_end)\n",
    "        sem_values1.append(sem1)  # Append the calculated SEM for variable 1\n",
    "        sem_values2.append(sem2)  # Append the calculated SEM for variable 2\n",
    "\n",
    "    # Calculate the difference between the bins for both variables\n",
    "    difference_values = [val1 - val2 for val1, val2 in zip(counts1, counts2)]\n",
    "\n",
    "    middle_of_bins = [start + bin_width_minutes / 2 for start in start_times_bin]\n",
    "\n",
    "    # Create a DataFrame to store the bin statistics for both variables and the difference\n",
    "    result_df = pd.DataFrame({\n",
    "        'Start Time': start_times_bin,\n",
    "        'End Time': end_times_bin,\n",
    "        f'Sum ({first_variable})': counts1,\n",
    "        f'Sum ({second_variable})': counts2,\n",
    "        f'Difference ({first_variable} - {second_variable})': difference_values,\n",
    "        f'SEM ({first_variable})': sem_values1,\n",
    "        f'SEM ({second_variable})': sem_values2\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(result_df)\n",
    "\n",
    "    # Create a plot of the difference between counts per minute with error bars representing SEM for both variables\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.errorbar(middle_of_bins, difference_values, yerr=sem_values1, marker='o', markersize=4, capsize=4, label=f'{first_variable} - {second_variable}')\n",
    "    plt.xlabel('Time (minutes)')\n",
    "    plt.ylabel(f'Difference ({first_variable} - {second_variable})')\n",
    "    plt.title(f'Difference between {first_variable} and {second_variable} per Minute Over Time ({bin_width_minutes}-minute Intervals) with SEM')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'matching_tuples' contains the selected tuples\n",
    "# and 'time_values' contains the corresponding time values\n",
    "\n",
    "# Define the selected indices for start and end time\n",
    "start_time_index = 5\n",
    "end_time_index = 6\n",
    "\n",
    "# Extract the name associated with the selected index\n",
    "selected_index = int(input(\"Select an index for the variables: \"))\n",
    "matching_tuples = []  # Initialize the list of matching tuples\n",
    "\n",
    "# Prompt the user to upload matching tuples and add them to the list\n",
    "while True:\n",
    "    user_choice = input(\"Do you want to upload a matching tuple? (yes/no): \")\n",
    "    if user_choice.lower() == \"yes\":\n",
    "        matching_tuple = tuple(float(x) for x in input(\"Enter values for the matching tuple separated by spaces: \").split())\n",
    "        matching_tuples.append(matching_tuple)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Process the matching tuples and plot the difference\n",
    "process_matching_tuples(matching_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [283]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# After filtering and obtaining matching_tuples, you can export them to a CSV file like this:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matching_tuples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from the list of matching tuples\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     matching_tuples_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatching_tuples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Create the full path to the CSV file in the specified directory\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     csv_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatching_tuples.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_directory = \"Z:\\\\KayCei\\\\Matching Tuples CSV\"  # Replace with the desired directory path\n",
    "\n",
    "# After filtering and obtaining matching_tuples, you can export them to a CSV file like this:\n",
    "if len(matching_tuples) > 0:\n",
    "    # Create a DataFrame from the list of matching tuples\n",
    "    matching_tuples_df = pd.DataFrame(matching_tuples)\n",
    "\n",
    "    # Create the full path to the CSV file in the specified directory\n",
    "    csv_filename = os.path.join(save_directory, \"matching_tuples.csv\")\n",
    "\n",
    "    # Export the DataFrame to the specified CSV file path\n",
    "    matching_tuples_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"Matching tuples have been exported to {csv_filename}.\")\n",
    "else:\n",
    "    print(\"No matching tuples to export.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONT NOT USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps is 30  1 behavior is 1/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_possible_indices(tuples):\n",
    "    if tuples and isinstance(tuples[0], tuple):\n",
    "        return range(len(tuples[0]))\n",
    "    return []\n",
    "\n",
    "while True:\n",
    "    start_filtering = input(\"Do you want to start filtering? (y/n): \")\n",
    "    if start_filtering.lower() != 'y':\n",
    "        print(\"Filtering process cancelled.\")\n",
    "        break\n",
    "\n",
    "    array_columns = [col for col in matching_rows.columns if \"array\" in col.lower()]\n",
    "\n",
    "    print(\"Available columns with 'array' in their names:\")\n",
    "    for idx, col in enumerate(array_columns):\n",
    "        print(f\"{idx}: {col}\")\n",
    "\n",
    "    selected_column_index = int(input(\"Enter the number of the column you want to use for values: \"))\n",
    "    selected_column_name = array_columns[selected_column_index]\n",
    "\n",
    "    if \"array\" in selected_column_name.lower():\n",
    "        tuple_cells = matching_rows[selected_column_name].tolist()\n",
    "\n",
    "        possible_indices = get_possible_indices(ast.literal_eval(tuple_cells[0]))\n",
    "        print(\"\\nAvailable indices for the elements within tuples:\")\n",
    "        print(possible_indices)\n",
    "\n",
    "        selected_index = int(input(\"Enter the index of the element you want to plot: \"))\n",
    "\n",
    "        if selected_index == 3:\n",
    "            print(\"Index 3 represents time, please select a different index.\")\n",
    "        elif selected_index==2:\n",
    "            generate_duration_plot = input(\"Do you want to generate a duration plot? (y/n): \")\n",
    "            if generate_duration_plot.lower() == 'y':\n",
    "                plt.figure()\n",
    "\n",
    "                durations = []\n",
    "                max_y_length = 0\n",
    "                \n",
    "                for cell in tuple_cells:\n",
    "                    tuple_data = ast.literal_eval(cell)\n",
    "                    if len(tuple_data) > selected_index and len(tuple_data) > 4:\n",
    "                        y_values = [x[selected_index] for x in tuple_data]\n",
    "\n",
    "                        if len(y_values) > max_y_length:\n",
    "                            max_y_length = len(y_values)\n",
    "\n",
    "                        durations.append(((tuple_data[-1][3] - tuple_data[0][1]) / 10))  # Calculate duration\n",
    "\n",
    "                x_values = list(range(1, max_y_length + 1))\n",
    "                avg_durations = np.zeros(max_y_length)\n",
    "\n",
    "                for duration in durations:\n",
    "                    for i in range(len(x_values)):\n",
    "                        if i < len(duration):\n",
    "                            avg_durations[i] += duration[i]\n",
    "\n",
    "                avg_durations /= len(durations)\n",
    "\n",
    "                plt.plot(x_values, avg_durations, label='Average Duration')\n",
    "                plt.xlabel('Index position')\n",
    "                plt.ylabel('Average Duration (seconds)')\n",
    "                plt.title(f'Average Duration Plot for Index {selected_index}')\n",
    "                plt.grid(False)\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "    continue_input = input(\"Do you want to continue? (y/n): \")\n",
    "    if continue_input.lower() == 'n':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bin_data(x_values, selected_index):\n",
    "    # Bin the data based on x values\n",
    "    bin_start = 0\n",
    "    bin_increment = 300\n",
    "    bins = np.arange(bin_start, max(x_values) + bin_increment, bin_increment)\n",
    "\n",
    "    # Create a histogram\n",
    "    hist, bin_edges = np.histogram(x_values, bins=bins)\n",
    "\n",
    "    return hist, bin_edges\n",
    "\n",
    "if x_values and y_values:\n",
    "    # Call the binning function\n",
    "    hist, bin_edges = bin_data(x_values, selected_index)\n",
    "    print(selected_index)\n",
    "    # Display the histogram\n",
    "    plt.figure()\n",
    "    plt.hist(x_values, bins=bin_edges, alpha=0.7)\n",
    "    plt.xlabel(f'Value at Index {selected_index}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Histogram of Values at Index {selected_index}')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No valid data found for plotting.\")\n",
    "   \n",
    "# Print the values for each bin\n",
    "for i, value in enumerate(hist):\n",
    "    bin_center = (bin_edges[i] + bin_edges[i + 1]) / 2\n",
    "    print(f\"Bin {i+1}: Range {bin_edges[i]} - {bin_edges[i+1]}, Value: {bin_center}, Frequency: {value}\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean and sem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add index for individual tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD6 on a given day what is the first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mask = []\n",
    "\n",
    "for i, x in enumerate(tuples_list):\n",
    "    found_true_for_index = False   # Reset the flag for each new row 'x'\n",
    "    \n",
    "    # Check if 'tup' is present in the current row 'x'\n",
    "    if tup in x:\n",
    "        if len(tup) > selected_index:\n",
    "            # Extract the value at 'selected_index'\n",
    "            value_at_selected_index = x[selected_index]\n",
    "            if range_min <= value_at_selected_index <= range_max:\n",
    "                mask.append(True)       # Append True to the mask\n",
    "                found_true_for_index = True  # Set flag to True for this index\n",
    "            else:\n",
    "                mask.append(False)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    else:\n",
    "        mask.append(False)\n",
    "\n",
    "    # Skip the rest of 'tup' once a True value is found for this index\n",
    "    if found_true_for_index:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Find columns with \"array\" in their names\n",
    "array_columns = [col for col in matching_rows.columns if \"array\" in col.lower()]\n",
    "print(\"Available columns with 'array' in their names:\")\n",
    "for idx, col in enumerate(array_columns):\n",
    "    print(f\"{idx}: {col}\")\n",
    "\n",
    "# Prompt user to select a column\n",
    "selected_column_index = int(input(\"Enter the number of the column you want to use for values: \"))\n",
    "selected_column_name = array_columns[selected_column_index]\n",
    "\n",
    "# Define the mapping of tuple indices to column names\n",
    "tuple_index_to_column = {\n",
    "    0: 'behavior',\n",
    "    1: 'duration/count',\n",
    "    2: 'bout',\n",
    "    3: 'time (deciseconds)'\n",
    "}\n",
    "\n",
    "for idx, col_name in tuple_index_to_column.items():\n",
    "    new_column_name = f\"{selected_column_name}_{col_name}\"\n",
    "    matching_rows.loc[:, new_column_name] = None  # Add new column with None values\n",
    "\n",
    "# Iterate through each row\n",
    "for index, row in matching_rows.iterrows():\n",
    "    array_data_str = row[selected_column_name]\n",
    "    # Check if the content is a valid list\n",
    "    try:\n",
    "        array_data = ast.literal_eval(array_data_str)\n",
    "        if not isinstance(array_data, list):\n",
    "            continue  # Skip this row if array_data is not a list\n",
    "    except (SyntaxError, ValueError, IndexError):\n",
    "        print(f'Error occurred while processing array_data in row {index}: {array_data_str}')\n",
    "        continue  # Skip this row if literal_eval encounters an error\n",
    "    \n",
    "    print(f\"Extracted array_data for row {index}: {array_data}\")  # Print the array_data\n",
    "    \n",
    "    # Process the list of tuples\n",
    "    for tuple_elements in array_data:\n",
    "        try:\n",
    "            if not isinstance(tuple_elements, tuple):\n",
    "                print(\"Error: This is not a tuple:\", tuple_elements)\n",
    "                continue\n",
    "                \n",
    "            processed_tuple = (float(tuple_elements[0]),) + tuple(map(int, tuple_elements[1:]))\n",
    "                \n",
    "            # Extract and update columns as before\n",
    "            for idx, element in enumerate(processed_tuple):\n",
    "                column_name = tuple_index_to_column.get(idx)\n",
    "                if column_name:\n",
    "                    new_column_name = f\"{selected_column_name}_{column_name}\"\n",
    "                    if matching_rows.at[index, new_column_name] is None:\n",
    "                        matching_rows.at[index, new_column_name] = []  # Initialize with empty list\n",
    "                    matching_rows.at[index, new_column_name].append(element)\n",
    "        except (ValueError):\n",
    "            print(\"A value error occurred while processing tuple_elements:\", tuple_elements)\n",
    "        except (SyntaxError):\n",
    "            print(\"A syntax error occurred while processing tuple_elements:\", tuple_elements)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(matching_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of newly added column names\n",
    "newly_added_columns = [f\"{selected_column_name}_{col_name}\" for col_name in tuple_index_to_column.values()]\n",
    "\n",
    "# Prompt user to select a column for binning\n",
    "print(\"Available columns for binning:\")\n",
    "for idx, col_name in enumerate(newly_added_columns):\n",
    "    print(f\"{idx}: {col_name}\")\n",
    "\n",
    "selected_binning_column_index = int(input(\"Enter the number of the column you want to use for bins: \"))\n",
    "\n",
    "if 0 <= selected_binning_column_index < len(newly_added_columns):\n",
    "    selected_binning_column_name = newly_added_columns[selected_binning_column_index]\n",
    "\n",
    "    # Create a dictionary to store values for each row\n",
    "    values_dict = {}\n",
    "\n",
    "    # Create a dictionary to store time values for each row\n",
    "    time_dictionary = {}\n",
    "\n",
    "    for idx, row in matching_rows.iterrows():\n",
    "        # Get the value of the selected column for the current row\n",
    "        value = row[selected_binning_column_name]\n",
    "\n",
    "        # Add the value to the values_dict for the current row\n",
    "        values_dict[idx] = value\n",
    "\n",
    "        # Get the time value for the current row\n",
    "        time_column = [col for col in row.index if \"time (deciseconds)\" in col.lower()]\n",
    "        if time_column:\n",
    "            time_value = row[time_column[0]]\n",
    "            time_dictionary[idx] = time_value\n",
    "\n",
    "    # Display the resulting dictionaries\n",
    "    print(\"Values Dictionary:\")\n",
    "    for row_index, value in values_dict.items():\n",
    "        print(f\"Row {row_index}: {value}\")\n",
    "\n",
    "    print(\"\\nTime Dictionary:\")\n",
    "    for row_index, time_value in time_dictionary.items():\n",
    "        print(f\"Row {row_index}: {time_value}\")\n",
    "else:\n",
    "    print(\"Invalid selection. Please choose a valid index.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate through values in the values_dict dictionary and retrieve corresponding time values\n",
    "print(\"Values and Time Dictionary Values:\")\n",
    "for row_index, value in values_dict.items():\n",
    "    time_value = time_dictionary.get(row_index)  # Retrieve time value using the same key\n",
    "    print(f\"Row {row_index}: Value: {value}, Time Value: {time_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3000,6000,9000,12000,15000,18000,21000,24000,27000,30000,31000,34000,39000,42000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = input(\"Enter the full path of the directory where you want to save the CSV file: \")\n",
    "output_filename = input(\"Enter the name of the CSV file to save the filtered data: \")\n",
    "output_file_path = f\"{output_path}/{output_filename}\"\n",
    "\n",
    "matching_rows.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Filtered data has been saved to {output_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to start filtering? (y/n): y\n",
      "Available columns with 'array' in their names:\n",
      "0: Right Turn Array\n",
      "1: Right Turn Array Continue\n",
      "2: Locomotion Array\n",
      "3: Locomotion Array Continue\n",
      "4: Face Groom Array\n",
      "5: Face Groom Array Continue\n",
      "Enter the number of the column you want to use for values: 0\n",
      "\n",
      "Available indices for the elements within tuples:\n",
      "[]\n",
      "Enter the index of the element you want to plot: 5\n",
      "Do you want to continue? (y/n): n\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_possible_indices(tuples):\n",
    "    index_to_label = {\n",
    "        0: \"Behavior,0\",\n",
    "        1: \"Stim Count,1\",\n",
    "        2: \"Frame Rate,2\",\n",
    "        3: \"Bouts,3\",\n",
    "        4: \"Duration,4\", \n",
    "        5: \"Start Time,5\", \n",
    "        6: \"End Time,6\"\n",
    "\n",
    "    }\n",
    "    if tuples and isinstance(tuples[0], tuple):\n",
    "        return [index_to_label[i] if i in index_to_label else f\"Unknown Label {i}\" for i in range(len(tuples[0]))]\n",
    "    return []\n",
    "\n",
    "while True:\n",
    "    start_filtering = input(\"Do you want to start filtering? (y/n): \")\n",
    "    if start_filtering.lower() != 'y':\n",
    "        print(\"Filtering process cancelled.\")\n",
    "        break\n",
    "\n",
    "    array_columns = [col for col in matching_rows.columns if \"array\" in col.lower()]\n",
    "\n",
    "    print(\"Available columns with 'array' in their names:\")\n",
    "    for idx, col in enumerate(array_columns):\n",
    "        print(f\"{idx}: {col}\")\n",
    "\n",
    "    selected_column_index = int(input(\"Enter the number of the column you want to use for values: \"))\n",
    "    selected_column_name = array_columns[selected_column_index]\n",
    "\n",
    "    if \"array\" in selected_column_name.lower():\n",
    "        tuple_cells = matching_rows[selected_column_name].tolist()\n",
    "\n",
    "        possible_indices = get_possible_indices(tuple_cells[0])\n",
    "        print(\"\\nAvailable indices for the elements within tuples:\")\n",
    "        print(possible_indices)\n",
    "\n",
    "        selected_index = int(input(\"Enter the index of the element you want to plot: \"))\n",
    "        \n",
    "        if selected_index == 3:\n",
    "            print(\"Index 3 represents time, please select a different index.\")\n",
    "        elif selected_index in possible_indices:\n",
    "            plt.figure()\n",
    "\n",
    "            for cell in tuple_cells:\n",
    "                x_values = []\n",
    "                y_values = []\n",
    "\n",
    "                tuple_data = ast.literal_eval(cell)\n",
    "                if len(tuple_data) > selected_index and len(tuple_data) > 7:\n",
    "                    for x in tuple_data:\n",
    "                        x_values.append(((x[3]-x[1])/10))  # Time at index 4\n",
    "                        y_values.append(x[selected_index])  # Convert y value to minutes\n",
    "\n",
    "                    if x_values and y_values:\n",
    "                        title = 'Bouts' if selected_index == 2 else 'Counts'\n",
    "                        plt.plot(x_values, y_values, label=f'Row {tuple_cells.index(cell) + 1}')\n",
    "            \n",
    "            plt.xlabel('Time (seconds)')\n",
    "            plt.ylabel(f'Value at Index {selected_index} (in minutes)')\n",
    "            plt.title(f'Combined Line Plot: Index {selected_index} vs. Time (Index 4) - {title}')\n",
    "            plt.grid(False)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    continue_input = input(\"Do you want to continue? (y/n): \")\n",
    "    if continue_input.lower() == 'n':\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
